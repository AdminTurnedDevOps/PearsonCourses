'use strict';

var Keyv = require('keyv');
var crypto = require('crypto');
var types = require('@backstage/types');
var errors = require('@backstage/errors');
var config = require('@backstage/config');
var knexFactory = require('knex');
var lodash = require('lodash');
var limiterFactory = require('p-limit');
var yn = require('yn');
var format = require('pg-format');
var backendDevUtils = require('@backstage/backend-dev-utils');
var fs = require('fs-extra');
var path = require('path');
var backendPluginApi = require('@backstage/backend-plugin-api');
var winston = require('winston');
var tripleBeam = require('triple-beam');
var configLoader = require('@backstage/config-loader');
var getPackages = require('@manypkg/get-packages');
var parseArgs = require('minimist');
var cliCommon = require('@backstage/cli-common');
var git = require('isomorphic-git');
var http = require('isomorphic-git/http/node');
var jose = require('jose');
var luxon = require('luxon');
var compression = require('compression');
var cors = require('cors');
var express = require('express');
var helmet = require('helmet');
var morgan = require('morgan');
var kebabCase = require('lodash/kebabCase');
var minimatch = require('minimatch');
var http$1 = require('http');
var https = require('https');
var stoppableServer = require('stoppable');
var forge = require('node-forge');
var Router = require('express-promise-router');
var stream = require('stream');
var clientNode = require('@kubernetes/client-node');
var uuid = require('uuid');
var Transport = require('winston-transport');
var pluginAuthNode = require('@backstage/plugin-auth-node');

function _interopDefaultCompat (e) { return e && typeof e === 'object' && 'default' in e ? e : { default: e }; }

function _interopNamespaceCompat(e) {
  if (e && typeof e === 'object' && 'default' in e) return e;
  var n = Object.create(null);
  if (e) {
    Object.keys(e).forEach(function (k) {
      if (k !== 'default') {
        var d = Object.getOwnPropertyDescriptor(e, k);
        Object.defineProperty(n, k, d.get ? d : {
          enumerable: true,
          get: function () { return e[k]; }
        });
      }
    });
  }
  n.default = e;
  return Object.freeze(n);
}

var Keyv__default = /*#__PURE__*/_interopDefaultCompat(Keyv);
var knexFactory__default = /*#__PURE__*/_interopDefaultCompat(knexFactory);
var limiterFactory__default = /*#__PURE__*/_interopDefaultCompat(limiterFactory);
var yn__default = /*#__PURE__*/_interopDefaultCompat(yn);
var format__default = /*#__PURE__*/_interopDefaultCompat(format);
var fs__default = /*#__PURE__*/_interopDefaultCompat(fs);
var path__default = /*#__PURE__*/_interopDefaultCompat(path);
var winston__namespace = /*#__PURE__*/_interopNamespaceCompat(winston);
var parseArgs__default = /*#__PURE__*/_interopDefaultCompat(parseArgs);
var git__default = /*#__PURE__*/_interopDefaultCompat(git);
var http__default = /*#__PURE__*/_interopDefaultCompat(http);
var compression__default = /*#__PURE__*/_interopDefaultCompat(compression);
var cors__default = /*#__PURE__*/_interopDefaultCompat(cors);
var express__default = /*#__PURE__*/_interopDefaultCompat(express);
var helmet__default = /*#__PURE__*/_interopDefaultCompat(helmet);
var morgan__default = /*#__PURE__*/_interopDefaultCompat(morgan);
var kebabCase__default = /*#__PURE__*/_interopDefaultCompat(kebabCase);
var http__namespace = /*#__PURE__*/_interopNamespaceCompat(http$1);
var https__namespace = /*#__PURE__*/_interopNamespaceCompat(https);
var stoppableServer__default = /*#__PURE__*/_interopDefaultCompat(stoppableServer);
var forge__default = /*#__PURE__*/_interopDefaultCompat(forge);
var Router__default = /*#__PURE__*/_interopDefaultCompat(Router);
var Transport__default = /*#__PURE__*/_interopDefaultCompat(Transport);

const DEFAULT_PORT = 7007;
const DEFAULT_HOST = "";
function readHttpServerOptions(config) {
  return {
    listen: readHttpListenOptions(config),
    https: readHttpsOptions(config)
  };
}
function readHttpListenOptions(config) {
  const listen = config?.getOptional("listen");
  if (typeof listen === "string") {
    const parts = String(listen).split(":");
    const port = parseInt(parts[parts.length - 1], 10);
    if (!isNaN(port)) {
      if (parts.length === 1) {
        return { port, host: DEFAULT_HOST };
      }
      if (parts.length === 2) {
        return { host: parts[0], port };
      }
    }
    throw new Error(
      `Unable to parse listen address ${listen}, expected <port> or <host>:<port>`
    );
  }
  const host = config?.getOptional("listen.host") ?? DEFAULT_HOST;
  if (typeof host !== "string") {
    config?.getOptionalString("listen.host");
    throw new Error("unreachable");
  }
  return {
    port: config?.getOptionalNumber("listen.port") ?? DEFAULT_PORT,
    host
  };
}
function readHttpsOptions(config) {
  const https = config?.getOptional("https");
  if (https === true) {
    const baseUrl = config.getString("baseUrl");
    let hostname;
    try {
      hostname = new URL(baseUrl).hostname;
    } catch (error) {
      throw new Error(`Invalid baseUrl "${baseUrl}"`);
    }
    return { certificate: { type: "generated", hostname } };
  }
  const cc = config?.getOptionalConfig("https");
  if (!cc) {
    return void 0;
  }
  return {
    certificate: {
      type: "pem",
      cert: cc.getString("certificate.cert"),
      key: cc.getString("certificate.key")
    }
  };
}

let HostDiscovery$1 = class HostDiscovery {
  constructor(internalBaseUrl, externalBaseUrl, discoveryConfig) {
    this.internalBaseUrl = internalBaseUrl;
    this.externalBaseUrl = externalBaseUrl;
    this.discoveryConfig = discoveryConfig;
  }
  /**
   * Creates a new HostDiscovery discovery instance by reading
   * from the `backend` config section, specifically the `.baseUrl` for
   * discovering the external URL, and the `.listen` and `.https` config
   * for the internal one.
   *
   * Can be overridden in config by providing a target and corresponding plugins in `discovery.endpoints`.
   * eg.
   *
   * ```yaml
   * discovery:
   *  endpoints:
   *    - target: https://internal.example.com/internal-catalog
   *      plugins: [catalog]
   *    - target: https://internal.example.com/secure/api/{{pluginId}}
   *      plugins: [auth, permission]
   *    - target:
   *        internal: https://internal.example.com/search
   *        external: https://example.com/search
   *      plugins: [search]
   * ```
   *
   * The fixed base path is `/api`, meaning the default full internal
   * path for the `catalog` plugin will be `http://localhost:7007/api/catalog`.
   */
  static fromConfig(config) {
    const basePath = "/api";
    const externalBaseUrl = config.getString("backend.baseUrl").replace(/\/+$/, "");
    const {
      listen: { host: listenHost = "::", port: listenPort }
    } = readHttpServerOptions(config.getConfig("backend"));
    const protocol = config.has("backend.https") ? "https" : "http";
    let host = listenHost;
    if (host === "::" || host === "") {
      host = "localhost";
    } else if (host === "0.0.0.0") {
      host = "127.0.0.1";
    }
    if (host.includes(":")) {
      host = `[${host}]`;
    }
    const internalBaseUrl = `${protocol}://${host}:${listenPort}`;
    return new HostDiscovery(
      internalBaseUrl + basePath,
      externalBaseUrl + basePath,
      config.getOptionalConfig("discovery")
    );
  }
  getTargetFromConfig(pluginId, type) {
    const endpoints = this.discoveryConfig?.getOptionalConfigArray("endpoints");
    const target = endpoints?.find((endpoint) => endpoint.getStringArray("plugins").includes(pluginId))?.get("target");
    if (!target) {
      const baseUrl = type === "external" ? this.externalBaseUrl : this.internalBaseUrl;
      return `${baseUrl}/${encodeURIComponent(pluginId)}`;
    }
    if (typeof target === "string") {
      return target.replace(
        /\{\{\s*pluginId\s*\}\}/g,
        encodeURIComponent(pluginId)
      );
    }
    return target[type].replace(
      /\{\{\s*pluginId\s*\}\}/g,
      encodeURIComponent(pluginId)
    );
  }
  async getBaseUrl(pluginId) {
    return this.getTargetFromConfig(pluginId, "internal");
  }
  async getExternalBaseUrl(pluginId) {
    return this.getTargetFromConfig(pluginId, "external");
  }
};

function ttlToMilliseconds(ttl) {
  return typeof ttl === "number" ? ttl : types.durationToMilliseconds(ttl);
}

class DefaultCacheClient {
  #client;
  #clientFactory;
  #options;
  constructor(client, clientFactory, options) {
    this.#client = client;
    this.#clientFactory = clientFactory;
    this.#options = options;
  }
  async get(key) {
    const k = this.getNormalizedKey(key);
    const value = await this.#client.get(k);
    return value;
  }
  async set(key, value, opts = {}) {
    const k = this.getNormalizedKey(key);
    const ttl = opts.ttl !== void 0 ? ttlToMilliseconds(opts.ttl) : void 0;
    await this.#client.set(k, value, ttl);
  }
  async delete(key) {
    const k = this.getNormalizedKey(key);
    await this.#client.delete(k);
  }
  withOptions(options) {
    const newOptions = { ...this.#options, ...options };
    return new DefaultCacheClient(
      this.#clientFactory(newOptions),
      this.#clientFactory,
      newOptions
    );
  }
  /**
   * Ensures keys are well-formed for any/all cache stores.
   */
  getNormalizedKey(candidateKey) {
    const wellFormedKey = Buffer.from(candidateKey).toString("base64");
    if (wellFormedKey.length < 200) {
      return wellFormedKey;
    }
    return crypto.createHash("sha256").update(candidateKey).digest("base64");
  }
}

let CacheManager$1 = class CacheManager {
  /**
   * Keys represent supported `backend.cache.store` values, mapped to factories
   * that return Keyv instances appropriate to the store.
   */
  storeFactories = {
    redis: this.createRedisStoreFactory(),
    memcache: this.createMemcacheStoreFactory(),
    memory: this.createMemoryStoreFactory()
  };
  logger;
  store;
  connection;
  useRedisSets;
  errorHandler;
  defaultTtl;
  /**
   * Creates a new {@link CacheManager} instance by reading from the `backend`
   * config section, specifically the `.cache` key.
   *
   * @param config - The loaded application configuration.
   */
  static fromConfig(config, options = {}) {
    const store = config.getOptionalString("backend.cache.store") || "memory";
    const defaultTtlConfig = config.getOptional("backend.cache.defaultTtl");
    const connectionString = config.getOptionalString("backend.cache.connection") || "";
    const useRedisSets = config.getOptionalBoolean("backend.cache.useRedisSets") ?? true;
    const logger = options.logger?.child({
      type: "cacheManager"
    });
    let defaultTtl;
    if (defaultTtlConfig !== void 0 && defaultTtlConfig !== null) {
      if (typeof defaultTtlConfig === "number") {
        defaultTtl = defaultTtlConfig;
      } else if (typeof defaultTtlConfig === "object" && !Array.isArray(defaultTtlConfig)) {
        defaultTtl = types.durationToMilliseconds(defaultTtlConfig);
      } else {
        throw new Error(
          `Invalid configuration backend.cache.defaultTtl: ${defaultTtlConfig}, expected milliseconds number or HumanDuration object`
        );
      }
    }
    return new CacheManager(
      store,
      connectionString,
      useRedisSets,
      options.onError,
      logger,
      defaultTtl
    );
  }
  /** @internal */
  constructor(store, connectionString, useRedisSets, errorHandler, logger, defaultTtl) {
    if (!this.storeFactories.hasOwnProperty(store)) {
      throw new Error(`Unknown cache store: ${store}`);
    }
    this.logger = logger;
    this.store = store;
    this.connection = connectionString;
    this.useRedisSets = useRedisSets;
    this.errorHandler = errorHandler;
    this.defaultTtl = defaultTtl;
  }
  /**
   * Generates a PluginCacheManager for consumption by plugins.
   *
   * @param pluginId - The plugin that the cache manager should be created for.
   *        Plugin names should be unique.
   */
  forPlugin(pluginId) {
    const clientFactory = (options) => {
      const ttl = options.defaultTtl ?? this.defaultTtl;
      return this.getClientWithTtl(
        pluginId,
        ttl !== void 0 ? ttlToMilliseconds(ttl) : void 0
      );
    };
    return new DefaultCacheClient(clientFactory({}), clientFactory, {});
  }
  getClientWithTtl(pluginId, ttl) {
    return this.storeFactories[this.store](pluginId, ttl);
  }
  createRedisStoreFactory() {
    const KeyvRedis = require("@keyv/redis");
    let store;
    return (pluginId, defaultTtl) => {
      if (!store) {
        store = new KeyvRedis(this.connection, {
          useRedisSets: this.useRedisSets
        });
        store.on("error", (err) => {
          this.logger?.error("Failed to create redis cache client", err);
          this.errorHandler?.(err);
        });
      }
      return new Keyv__default.default({
        namespace: pluginId,
        ttl: defaultTtl,
        store,
        emitErrors: false,
        useRedisSets: this.useRedisSets
      });
    };
  }
  createMemcacheStoreFactory() {
    const KeyvMemcache = require("@keyv/memcache");
    let store;
    return (pluginId, defaultTtl) => {
      if (!store) {
        store = new KeyvMemcache(this.connection);
        store.on("error", (err) => {
          this.logger?.error("Failed to create memcache cache client", err);
          this.errorHandler?.(err);
        });
      }
      return new Keyv__default.default({
        namespace: pluginId,
        ttl: defaultTtl,
        emitErrors: false,
        store
      });
    };
  }
  createMemoryStoreFactory() {
    const store = /* @__PURE__ */ new Map();
    return (pluginId, defaultTtl) => new Keyv__default.default({
      namespace: pluginId,
      ttl: defaultTtl,
      emitErrors: false,
      store
    });
  }
};

function defaultNameOverride(name) {
  return {
    connection: {
      database: name
    }
  };
}

function mergeDatabaseConfig(config, ...overrides) {
  return lodash.merge({}, config, ...overrides);
}

const ddlLimiter$1 = limiterFactory__default.default(1);
function createMysqlDatabaseClient(dbConfig, overrides) {
  const knexConfig = buildMysqlDatabaseConfig(dbConfig, overrides);
  const database = knexFactory__default.default(knexConfig);
  return database;
}
function buildMysqlDatabaseConfig(dbConfig, overrides) {
  return mergeDatabaseConfig(
    dbConfig.get(),
    {
      connection: getMysqlConnectionConfig(dbConfig, !!overrides),
      useNullAsDefault: true
    },
    overrides
  );
}
function getMysqlConnectionConfig(dbConfig, parseConnectionString) {
  const connection = dbConfig.get("connection");
  const isConnectionString = typeof connection === "string" || connection instanceof String;
  const autoParse = typeof parseConnectionString !== "boolean";
  const shouldParseConnectionString = autoParse ? isConnectionString : parseConnectionString && isConnectionString;
  return shouldParseConnectionString ? parseMysqlConnectionString(connection) : connection;
}
function parseMysqlConnectionString(connectionString) {
  try {
    const {
      protocol,
      username,
      password,
      port,
      hostname,
      pathname,
      searchParams
    } = new URL(connectionString);
    if (protocol !== "mysql:") {
      throw new Error(`Unknown protocol ${protocol}`);
    } else if (!username || !password) {
      throw new Error(`Missing username/password`);
    } else if (!pathname.match(/^\/[^/]+$/)) {
      throw new Error(`Expected single path segment`);
    }
    const result = {
      user: username,
      password,
      host: hostname,
      port: Number(port || 3306),
      database: decodeURIComponent(pathname.substring(1))
    };
    const ssl = searchParams.get("ssl");
    if (ssl) {
      result.ssl = ssl;
    }
    const debug = searchParams.get("debug");
    if (debug) {
      result.debug = yn__default.default(debug);
    }
    return result;
  } catch (e) {
    throw new errors.InputError(
      `Error while parsing MySQL connection string, ${e}`,
      e
    );
  }
}
async function ensureMysqlDatabaseExists(dbConfig, ...databases) {
  const admin = createMysqlDatabaseClient(dbConfig, {
    connection: {
      database: null
    },
    pool: {
      min: 0,
      acquireTimeoutMillis: 1e4
    }
  });
  try {
    const ensureDatabase = async (database) => {
      await admin.raw(`CREATE DATABASE IF NOT EXISTS ??`, [database]);
    };
    await Promise.all(
      databases.map(async (database) => {
        let lastErr = void 0;
        for (let i = 0; i < 3; i++) {
          try {
            return await ddlLimiter$1(() => ensureDatabase(database));
          } catch (err) {
            lastErr = err;
          }
          await new Promise((resolve) => setTimeout(resolve, 100));
        }
        throw lastErr;
      })
    );
  } finally {
    await admin.destroy();
  }
}
function pluginPath$3(pluginId) {
  return `plugin.${pluginId}`;
}
function normalizeConnection$2(connection) {
  if (typeof connection === "undefined" || connection === null) {
    return {};
  }
  return typeof connection === "string" || connection instanceof String ? parseMysqlConnectionString(connection) : connection;
}
class MysqlConnector {
  constructor(config, prefix) {
    this.config = config;
    this.prefix = prefix;
  }
  async getClient(pluginId, _deps) {
    const pluginConfig = new config.ConfigReader(
      this.getConfigForPlugin(pluginId)
    );
    const databaseName = this.getDatabaseName(pluginId);
    if (databaseName && this.getEnsureExistsConfig(pluginId)) {
      try {
        await ensureMysqlDatabaseExists(pluginConfig, databaseName);
      } catch (error) {
        throw new Error(
          `Failed to connect to the database to make sure that '${databaseName}' exists, ${error}`
        );
      }
    }
    const pluginDivisionMode = this.getPluginDivisionModeConfig();
    if (pluginDivisionMode !== "database") {
      throw new Error(
        `The MySQL driver does not support plugin division mode '${pluginDivisionMode}'`
      );
    }
    const databaseClientOverrides = mergeDatabaseConfig(
      {},
      this.getDatabaseOverrides(pluginId)
    );
    const client = createMysqlDatabaseClient(
      pluginConfig,
      databaseClientOverrides
    );
    return client;
  }
  /**
   * Provides the canonical database name for a given plugin.
   *
   * This method provides the effective database name which is determined using
   * global and plugin specific database config. If no explicit database name,
   * this method will provide a generated name which is the pluginId prefixed
   * with 'backstage_plugin_'.
   *
   * @param pluginId - Lookup the database name for given plugin
   * @returns String representing the plugin's database name
   */
  getDatabaseName(pluginId) {
    const connection = this.getConnectionConfig(pluginId);
    const databaseName = connection?.database;
    return databaseName ?? `${this.prefix}${pluginId}`;
  }
  /**
   * Provides the client type which should be used for a given plugin.
   *
   * The client type is determined by plugin specific config if present.
   * Otherwise the base client is used as the fallback.
   *
   * @param pluginId - Plugin to get the client type for
   * @returns Object with client type returned as `client` and boolean
   *          representing whether or not the client was overridden as
   *          `overridden`
   */
  getClientType(pluginId) {
    const pluginClient = this.config.getOptionalString(
      `${pluginPath$3(pluginId)}.client`
    );
    const baseClient = this.config.getString("client");
    const client = pluginClient ?? baseClient;
    return {
      client,
      overridden: client !== baseClient
    };
  }
  getRoleConfig(pluginId) {
    return this.config.getOptionalString(`${pluginPath$3(pluginId)}.role`) ?? this.config.getOptionalString("role");
  }
  /**
   * Provides the knexConfig which should be used for a given plugin.
   *
   * @param pluginId - Plugin to get the knexConfig for
   * @returns The merged knexConfig value or undefined if it isn't specified
   */
  getAdditionalKnexConfig(pluginId) {
    const pluginConfig = this.config.getOptionalConfig(`${pluginPath$3(pluginId)}.knexConfig`)?.get();
    const baseConfig = this.config.getOptionalConfig("knexConfig")?.get();
    return lodash.merge(baseConfig, pluginConfig);
  }
  getEnsureExistsConfig(pluginId) {
    const baseConfig = this.config.getOptionalBoolean("ensureExists") ?? true;
    return this.config.getOptionalBoolean(`${pluginPath$3(pluginId)}.ensureExists`) ?? baseConfig;
  }
  getPluginDivisionModeConfig() {
    return this.config.getOptionalString("pluginDivisionMode") ?? "database";
  }
  /**
   * Provides a Knex connection plugin config by combining base and plugin
   * config.
   *
   * This method provides a baseConfig for a plugin database connector. If the
   * client type has not been overridden, the global connection config will be
   * included with plugin specific config as the base. Values from the plugin
   * connection take precedence over the base. Base database name is omitted
   * unless `pluginDivisionMode` is set to `schema`.
   */
  getConnectionConfig(pluginId) {
    const { overridden } = this.getClientType(pluginId);
    let baseConnection = normalizeConnection$2(this.config.get("connection"));
    if (this.getPluginDivisionModeConfig() !== "schema") {
      baseConnection = lodash.omit(baseConnection, "database");
    }
    const connection = normalizeConnection$2(
      this.config.getOptional(`${pluginPath$3(pluginId)}.connection`)
    );
    return {
      // include base connection if client type has not been overridden
      ...overridden ? {} : baseConnection,
      ...connection
    };
  }
  /**
   * Provides a Knex database config for a given plugin.
   *
   * This method provides a Knex configuration object along with the plugin's
   * client type.
   *
   * @param pluginId - The plugin that the database config should correspond with
   */
  getConfigForPlugin(pluginId) {
    const { client } = this.getClientType(pluginId);
    const role = this.getRoleConfig(pluginId);
    return {
      ...this.getAdditionalKnexConfig(pluginId),
      client,
      connection: this.getConnectionConfig(pluginId),
      ...role && { role }
    };
  }
  /**
   * Provides a partial `Knex.Config`• database name override for a given plugin.
   *
   * @param pluginId - Target plugin to get database name override
   * @returns Partial `Knex.Config` with database name override
   */
  getDatabaseOverrides(pluginId) {
    const databaseName = this.getDatabaseName(pluginId);
    return databaseName ? defaultNameOverride(databaseName) : {};
  }
}

function defaultSchemaOverride(name) {
  return {
    searchPath: [name]
  };
}

const ddlLimiter = limiterFactory__default.default(1);
function createPgDatabaseClient(dbConfig, overrides) {
  const knexConfig = buildPgDatabaseConfig(dbConfig, overrides);
  const database = knexFactory__default.default(knexConfig);
  const role = dbConfig.getOptionalString("role");
  if (role) {
    database.client.pool.on(
      "createSuccess",
      async (_event, pgClient) => {
        const query = format__default.default("SET ROLE %I", role);
        await pgClient.query(query);
      }
    );
  }
  return database;
}
function buildPgDatabaseConfig(dbConfig, overrides) {
  return mergeDatabaseConfig(
    dbConfig.get(),
    {
      connection: getPgConnectionConfig(dbConfig, !!overrides),
      useNullAsDefault: true
    },
    overrides
  );
}
function getPgConnectionConfig(dbConfig, parseConnectionString) {
  const connection = dbConfig.get("connection");
  const isConnectionString = typeof connection === "string" || connection instanceof String;
  const autoParse = typeof parseConnectionString !== "boolean";
  const shouldParseConnectionString = autoParse ? isConnectionString : parseConnectionString && isConnectionString;
  return shouldParseConnectionString ? parsePgConnectionString(connection) : connection;
}
function parsePgConnectionString(connectionString) {
  const parse = requirePgConnectionString();
  return parse(connectionString);
}
function requirePgConnectionString() {
  try {
    return require("pg-connection-string").parse;
  } catch (e) {
    throw new errors.ForwardedError("Postgres: Install 'pg-connection-string'", e);
  }
}
async function ensurePgDatabaseExists(dbConfig, ...databases) {
  const admin = createPgDatabaseClient(dbConfig, {
    connection: {
      database: "postgres"
    },
    pool: {
      min: 0,
      acquireTimeoutMillis: 1e4
    }
  });
  try {
    const ensureDatabase = async (database) => {
      const result = await admin.from("pg_database").where("datname", database).count();
      if (parseInt(result[0].count, 10) > 0) {
        return;
      }
      await admin.raw(`CREATE DATABASE ??`, [database]);
    };
    await Promise.all(
      databases.map(async (database) => {
        let lastErr = void 0;
        for (let i = 0; i < 3; i++) {
          try {
            return await ddlLimiter(() => ensureDatabase(database));
          } catch (err) {
            lastErr = err;
          }
          await new Promise((resolve) => setTimeout(resolve, 100));
        }
        throw lastErr;
      })
    );
  } finally {
    await admin.destroy();
  }
}
async function ensurePgSchemaExists(dbConfig, ...schemas) {
  const admin = createPgDatabaseClient(dbConfig);
  const role = dbConfig.getOptionalString("role");
  try {
    const ensureSchema = async (database) => {
      if (role) {
        await admin.raw(`CREATE SCHEMA IF NOT EXISTS ?? AUTHORIZATION ??`, [
          database,
          role
        ]);
      } else {
        await admin.raw(`CREATE SCHEMA IF NOT EXISTS ??`, [database]);
      }
    };
    await Promise.all(
      schemas.map((database) => ddlLimiter(() => ensureSchema(database)))
    );
  } finally {
    await admin.destroy();
  }
}
function pluginPath$2(pluginId) {
  return `plugin.${pluginId}`;
}
function normalizeConnection$1(connection) {
  if (typeof connection === "undefined" || connection === null) {
    return {};
  }
  return typeof connection === "string" || connection instanceof String ? parsePgConnectionString(connection) : connection;
}
class PgConnector {
  constructor(config, prefix) {
    this.config = config;
    this.prefix = prefix;
  }
  async getClient(pluginId, _deps) {
    const pluginConfig = new config.ConfigReader(
      this.getConfigForPlugin(pluginId)
    );
    const databaseName = this.getDatabaseName(pluginId);
    if (databaseName && this.getEnsureExistsConfig(pluginId)) {
      try {
        await ensurePgDatabaseExists(pluginConfig, databaseName);
      } catch (error) {
        throw new Error(
          `Failed to connect to the database to make sure that '${databaseName}' exists, ${error}`
        );
      }
    }
    let schemaOverrides;
    if (this.getPluginDivisionModeConfig() === "schema") {
      schemaOverrides = defaultSchemaOverride(pluginId);
      if (this.getEnsureSchemaExistsConfig(pluginId) || this.getEnsureExistsConfig(pluginId)) {
        try {
          await ensurePgSchemaExists(pluginConfig, pluginId);
        } catch (error) {
          throw new Error(
            `Failed to connect to the database to make sure that schema for plugin '${pluginId}' exists, ${error}`
          );
        }
      }
    }
    const databaseClientOverrides = mergeDatabaseConfig(
      {},
      this.getDatabaseOverrides(pluginId),
      schemaOverrides
    );
    const client = createPgDatabaseClient(
      pluginConfig,
      databaseClientOverrides
    );
    return client;
  }
  /**
   * Provides the canonical database name for a given plugin.
   *
   * This method provides the effective database name which is determined using global
   * and plugin specific database config. If no explicit database name is configured
   * and `pluginDivisionMode` is not `schema`, this method will provide a generated name
   * which is the pluginId prefixed with 'backstage_plugin_'. If `pluginDivisionMode` is
   * `schema`, it will fallback to using the default database for the knex instance.
   *
   * @param pluginId - Lookup the database name for given plugin
   * @returns String representing the plugin's database name
   */
  getDatabaseName(pluginId) {
    const connection = this.getConnectionConfig(pluginId);
    const databaseName = connection?.database;
    if (this.getPluginDivisionModeConfig() === "schema") {
      return databaseName;
    }
    return databaseName ?? `${this.prefix}${pluginId}`;
  }
  /**
   * Provides the client type which should be used for a given plugin.
   *
   * The client type is determined by plugin specific config if present.
   * Otherwise the base client is used as the fallback.
   *
   * @param pluginId - Plugin to get the client type for
   * @returns Object with client type returned as `client` and boolean
   *          representing whether or not the client was overridden as
   *          `overridden`
   */
  getClientType(pluginId) {
    const pluginClient = this.config.getOptionalString(
      `${pluginPath$2(pluginId)}.client`
    );
    const baseClient = this.config.getString("client");
    const client = pluginClient ?? baseClient;
    return {
      client,
      overridden: client !== baseClient
    };
  }
  getRoleConfig(pluginId) {
    return this.config.getOptionalString(`${pluginPath$2(pluginId)}.role`) ?? this.config.getOptionalString("role");
  }
  /**
   * Provides the knexConfig which should be used for a given plugin.
   *
   * @param pluginId - Plugin to get the knexConfig for
   * @returns The merged knexConfig value or undefined if it isn't specified
   */
  getAdditionalKnexConfig(pluginId) {
    const pluginConfig = this.config.getOptionalConfig(`${pluginPath$2(pluginId)}.knexConfig`)?.get();
    const baseConfig = this.config.getOptionalConfig("knexConfig")?.get();
    return lodash.merge(baseConfig, pluginConfig);
  }
  getEnsureExistsConfig(pluginId) {
    const baseConfig = this.config.getOptionalBoolean("ensureExists") ?? true;
    return this.config.getOptionalBoolean(`${pluginPath$2(pluginId)}.ensureExists`) ?? baseConfig;
  }
  getEnsureSchemaExistsConfig(pluginId) {
    const baseConfig = this.config.getOptionalBoolean("ensureSchemaExists") ?? false;
    return this.config.getOptionalBoolean(
      `${pluginPath$2(pluginId)}.getEnsureSchemaExistsConfig`
    ) ?? baseConfig;
  }
  getPluginDivisionModeConfig() {
    return this.config.getOptionalString("pluginDivisionMode") ?? "database";
  }
  /**
   * Provides a Knex connection plugin config by combining base and plugin
   * config.
   *
   * This method provides a baseConfig for a plugin database connector. If the
   * client type has not been overridden, the global connection config will be
   * included with plugin specific config as the base. Values from the plugin
   * connection take precedence over the base. Base database name is omitted
   * unless `pluginDivisionMode` is set to `schema`.
   */
  getConnectionConfig(pluginId) {
    const { overridden } = this.getClientType(pluginId);
    let baseConnection = normalizeConnection$1(this.config.get("connection"));
    if (this.getPluginDivisionModeConfig() !== "schema") {
      baseConnection = lodash.omit(baseConnection, "database");
    }
    const connection = normalizeConnection$1(
      this.config.getOptional(`${pluginPath$2(pluginId)}.connection`)
    );
    baseConnection.application_name ||= `backstage_plugin_${pluginId}`;
    return {
      // include base connection if client type has not been overridden
      ...overridden ? {} : baseConnection,
      ...connection
    };
  }
  /**
   * Provides a Knex database config for a given plugin.
   *
   * This method provides a Knex configuration object along with the plugin's
   * client type.
   *
   * @param pluginId - The plugin that the database config should correspond with
   */
  getConfigForPlugin(pluginId) {
    const { client } = this.getClientType(pluginId);
    const role = this.getRoleConfig(pluginId);
    return {
      ...this.getAdditionalKnexConfig(pluginId),
      client,
      connection: this.getConnectionConfig(pluginId),
      ...role && { role }
    };
  }
  /**
   * Provides a partial `Knex.Config`• database name override for a given plugin.
   *
   * @param pluginId - Target plugin to get database name override
   * @returns Partial `Knex.Config` with database name override
   */
  getDatabaseOverrides(pluginId) {
    const databaseName = this.getDatabaseName(pluginId);
    return databaseName ? defaultNameOverride(databaseName) : {};
  }
}

function createSqliteDatabaseClient(pluginId, dbConfig, deps, overrides) {
  const knexConfig = buildSqliteDatabaseConfig(dbConfig, overrides);
  const connConfig = knexConfig.connection;
  const filename = connConfig.filename ?? ":memory:";
  if (filename !== ":memory:") {
    const directory = path__default.default.dirname(filename);
    fs.ensureDirSync(directory);
  }
  let database;
  if (deps && filename === ":memory:") {
    const devStore = backendDevUtils.DevDataStore.get();
    if (devStore) {
      const dataKey = `sqlite3-db-${pluginId}`;
      const connectionLoader = async () => {
        const { data: seedData } = await devStore.load(dataKey);
        return {
          ...knexConfig.connection,
          filename: seedData ?? ":memory:"
        };
      };
      database = knexFactory__default.default({
        ...knexConfig,
        connection: Object.assign(connectionLoader, {
          // This is a workaround for the knex SQLite driver always warning when using a config loader
          filename: ":memory:"
        })
      });
      deps.lifecycle.addShutdownHook(async () => {
        const connection = await database.client.acquireConnection();
        const data = connection.serialize();
        await devStore.save(dataKey, data);
      });
    } else {
      database = knexFactory__default.default(knexConfig);
    }
  } else {
    database = knexFactory__default.default(knexConfig);
  }
  database.client.pool.on("createSuccess", (_eventId, resource) => {
    resource.run("PRAGMA foreign_keys = ON", () => {
    });
  });
  return database;
}
function buildSqliteDatabaseConfig(dbConfig, overrides) {
  const baseConfig = dbConfig.get();
  if (typeof baseConfig.connection === "string") {
    baseConfig.connection = { filename: baseConfig.connection };
  }
  if (overrides && typeof overrides.connection === "string") {
    overrides.connection = { filename: overrides.connection };
  }
  const config = mergeDatabaseConfig(
    {
      connection: {}
    },
    baseConfig,
    {
      useNullAsDefault: true
    },
    overrides
  );
  return config;
}
function createSqliteNameOverride(name) {
  return {
    connection: parseSqliteConnectionString(name)
  };
}
function parseSqliteConnectionString(name) {
  return {
    filename: name
  };
}
function pluginPath$1(pluginId) {
  return `plugin.${pluginId}`;
}
function normalizeConnection(connection) {
  if (typeof connection === "undefined" || connection === null) {
    return {};
  }
  return typeof connection === "string" || connection instanceof String ? parseSqliteConnectionString(connection) : connection;
}
class Sqlite3Connector {
  constructor(config) {
    this.config = config;
  }
  async getClient(pluginId, deps) {
    const pluginConfig = new config.ConfigReader(
      this.getConfigForPlugin(pluginId)
    );
    const pluginDivisionMode = this.getPluginDivisionModeConfig();
    if (pluginDivisionMode !== "database") {
      throw new Error(
        `The SQLite driver does not support plugin division mode '${pluginDivisionMode}'`
      );
    }
    const databaseClientOverrides = mergeDatabaseConfig(
      {},
      this.getDatabaseOverrides(pluginId)
    );
    const client = createSqliteDatabaseClient(
      pluginId,
      pluginConfig,
      deps,
      databaseClientOverrides
    );
    return client;
  }
  /**
   * Provides the canonical database name for a given plugin.
   *
   * This method provides the effective database name which is determined using global
   * and plugin specific database config. If no explicit database name is configured
   * and `pluginDivisionMode` is not `schema`, this method will provide a generated name
   * which is the pluginId prefixed with 'backstage_plugin_'. If `pluginDivisionMode` is
   * `schema`, it will fallback to using the default database for the knex instance.
   *
   * @param pluginId - Lookup the database name for given plugin
   * @returns String representing the plugin's database name
   */
  getDatabaseName(pluginId) {
    const connection = this.getConnectionConfig(pluginId);
    const sqliteFilename = connection.filename;
    if (sqliteFilename === ":memory:") {
      return sqliteFilename;
    }
    const sqliteDirectory = connection.directory ?? ".";
    return path__default.default.join(sqliteDirectory, sqliteFilename ?? `${pluginId}.sqlite`);
  }
  /**
   * Provides the client type which should be used for a given plugin.
   *
   * The client type is determined by plugin specific config if present.
   * Otherwise the base client is used as the fallback.
   *
   * @param pluginId - Plugin to get the client type for
   * @returns Object with client type returned as `client` and boolean
   *          representing whether or not the client was overridden as
   *          `overridden`
   */
  getClientType(pluginId) {
    const pluginClient = this.config.getOptionalString(
      `${pluginPath$1(pluginId)}.client`
    );
    const baseClient = this.config.getString("client");
    const client = pluginClient ?? baseClient;
    return {
      client,
      overridden: client !== baseClient
    };
  }
  getRoleConfig(pluginId) {
    return this.config.getOptionalString(`${pluginPath$1(pluginId)}.role`) ?? this.config.getOptionalString("role");
  }
  /**
   * Provides the knexConfig which should be used for a given plugin.
   *
   * @param pluginId - Plugin to get the knexConfig for
   * @returns The merged knexConfig value or undefined if it isn't specified
   */
  getAdditionalKnexConfig(pluginId) {
    const pluginConfig = this.config.getOptionalConfig(`${pluginPath$1(pluginId)}.knexConfig`)?.get();
    const baseConfig = this.config.getOptionalConfig("knexConfig")?.get();
    return lodash.merge(baseConfig, pluginConfig);
  }
  getPluginDivisionModeConfig() {
    return this.config.getOptionalString("pluginDivisionMode") ?? "database";
  }
  /**
   * Provides a Knex connection plugin config by combining base and plugin
   * config.
   *
   * This method provides a baseConfig for a plugin database connector. If the
   * client type has not been overridden, the global connection config will be
   * included with plugin specific config as the base. Values from the plugin
   * connection take precedence over the base. Base database name is omitted for
   * all supported databases excluding SQLite unless `pluginDivisionMode` is set
   * to `schema`.
   */
  getConnectionConfig(pluginId) {
    const { client, overridden } = this.getClientType(pluginId);
    let baseConnection = normalizeConnection(this.config.get("connection"));
    if (client.includes("sqlite3") && "filename" in baseConnection && baseConnection.filename !== ":memory:") {
      throw new Error(
        "`connection.filename` is not supported for the base sqlite connection. Prefer `connection.directory` or provide a filename for the plugin connection instead."
      );
    }
    if (this.getPluginDivisionModeConfig() !== "schema") {
      baseConnection = lodash.omit(baseConnection, "database");
    }
    const connection = normalizeConnection(
      this.config.getOptional(`${pluginPath$1(pluginId)}.connection`)
    );
    return {
      // include base connection if client type has not been overridden
      ...overridden ? {} : baseConnection,
      ...connection
    };
  }
  /**
   * Provides a Knex database config for a given plugin.
   *
   * This method provides a Knex configuration object along with the plugin's
   * client type.
   *
   * @param pluginId - The plugin that the database config should correspond with
   */
  getConfigForPlugin(pluginId) {
    const { client } = this.getClientType(pluginId);
    const role = this.getRoleConfig(pluginId);
    return {
      ...this.getAdditionalKnexConfig(pluginId),
      client,
      connection: this.getConnectionConfig(pluginId),
      ...role && { role }
    };
  }
  /**
   * Provides a partial `Knex.Config`• database name override for a given plugin.
   *
   * @param pluginId - Target plugin to get database name override
   * @returns Partial `Knex.Config` with database name override
   */
  getDatabaseOverrides(pluginId) {
    const databaseName = this.getDatabaseName(pluginId);
    return databaseName ? createSqliteNameOverride(databaseName) : {};
  }
}

function pluginPath(pluginId) {
  return `plugin.${pluginId}`;
}
class DatabaseManagerImpl {
  constructor(config, connectors, options, databaseCache = /* @__PURE__ */ new Map()) {
    this.config = config;
    this.connectors = connectors;
    this.options = options;
    this.databaseCache = databaseCache;
  }
  /**
   * Generates a PluginDatabaseManager for consumption by plugins.
   *
   * @param pluginId - The plugin that the database manager should be created for. Plugin names
   * should be unique as they are used to look up database config overrides under
   * `backend.database.plugin`.
   */
  forPlugin(pluginId, deps) {
    const client = this.getClientType(pluginId).client;
    const connector = this.connectors[client];
    if (!connector) {
      throw new Error(
        `Unsupported database client type '${client}' specified for plugin '${pluginId}'`
      );
    }
    const getClient = () => this.getDatabase(pluginId, connector, deps);
    const skip = this.options?.migrations?.skip ?? this.config.getOptionalBoolean(
      `backend.database.plugin.${pluginId}.skipMigrations`
    ) ?? this.config.getOptionalBoolean("backend.database.skipMigrations") ?? false;
    return { getClient, migrations: { skip } };
  }
  /**
   * Provides the client type which should be used for a given plugin.
   *
   * The client type is determined by plugin specific config if present.
   * Otherwise the base client is used as the fallback.
   *
   * @param pluginId - Plugin to get the client type for
   * @returns Object with client type returned as `client` and boolean
   *          representing whether or not the client was overridden as
   *          `overridden`
   */
  getClientType(pluginId) {
    const pluginClient = this.config.getOptionalString(
      `${pluginPath(pluginId)}.client`
    );
    const baseClient = this.config.getString("client");
    const client = pluginClient ?? baseClient;
    return {
      client,
      overridden: client !== baseClient
    };
  }
  /**
   * Provides a scoped Knex client for a plugin as per application config.
   *
   * @param pluginId - Plugin to get a Knex client for
   * @returns Promise which resolves to a scoped Knex database client for a
   *          plugin
   */
  async getDatabase(pluginId, connector, deps) {
    if (this.databaseCache.has(pluginId)) {
      return this.databaseCache.get(pluginId);
    }
    const clientPromise = connector.getClient(pluginId, deps);
    this.databaseCache.set(pluginId, clientPromise);
    if (process.env.NODE_ENV !== "test") {
      clientPromise.then(
        (client) => this.startKeepaliveLoop(pluginId, client, deps.logger)
      );
    }
    return clientPromise;
  }
  startKeepaliveLoop(pluginId, client, logger) {
    let lastKeepaliveFailed = false;
    setInterval(() => {
      client?.raw("select 1").then(
        () => {
          lastKeepaliveFailed = false;
        },
        (error) => {
          if (!lastKeepaliveFailed) {
            lastKeepaliveFailed = true;
            logger.warn(
              `Database keepalive failed for plugin ${pluginId}, ${errors.stringifyError(
                error
              )}`
            );
          }
        }
      );
    }, 60 * 1e3);
  }
}
let DatabaseManager$1 = class DatabaseManager {
  constructor(impl) {
    this.impl = impl;
  }
  /**
   * Creates a {@link DatabaseManager} from `backend.database` config.
   *
   * @param config - The loaded application configuration.
   * @param options - An optional configuration object.
   */
  static fromConfig(config, options) {
    const databaseConfig = config.getConfig("backend.database");
    const prefix = databaseConfig.getOptionalString("prefix") || "backstage_plugin_";
    return new DatabaseManager(
      new DatabaseManagerImpl(
        databaseConfig,
        {
          pg: new PgConnector(databaseConfig, prefix),
          sqlite3: new Sqlite3Connector(databaseConfig),
          "better-sqlite3": new Sqlite3Connector(databaseConfig),
          mysql: new MysqlConnector(databaseConfig, prefix),
          mysql2: new MysqlConnector(databaseConfig, prefix)
        },
        options
      )
    );
  }
  /**
   * Generates a PluginDatabaseManager for consumption by plugins.
   *
   * @param pluginId - The plugin that the database manager should be created for. Plugin names
   * should be unique as they are used to look up database config overrides under
   * `backend.database.plugin`.
   */
  forPlugin(pluginId, deps) {
    return this.impl.forPlugin(pluginId, deps);
  }
};

function findAllAncestors(_module) {
  const ancestors = new Array();
  const parentIds = /* @__PURE__ */ new Set();
  function add(id, m) {
    if (parentIds.has(id)) {
      return;
    }
    parentIds.add(id);
    ancestors.push(m);
    for (const parentId of m.parents) {
      const parent = require.cache[parentId];
      if (parent) {
        add(parentId, parent);
      }
    }
  }
  add(_module.id, _module);
  return ancestors;
}
function useHotCleanup(_module, cancelEffect) {
  if (_module.hot) {
    const ancestors = findAllAncestors(_module);
    let cancelled = false;
    const handler = () => {
      if (!cancelled) {
        cancelled = true;
        cancelEffect();
      }
    };
    for (const m of ancestors) {
      m.hot?.addDisposeHandler(handler);
    }
  }
}
const CURRENT_HOT_MEMOIZE_INDEX_KEY = "backstage.io/hmr-memoize-key";
function useHotMemoize(_module, valueFactory) {
  if (!_module.hot) {
    return valueFactory();
  }
  if (!_module.hot.data?.[CURRENT_HOT_MEMOIZE_INDEX_KEY]) {
    for (const ancestor of findAllAncestors(_module)) {
      ancestor.hot?.addDisposeHandler((data) => {
        data[CURRENT_HOT_MEMOIZE_INDEX_KEY] = 1;
      });
    }
    _module.hot.data = {
      ..._module.hot.data,
      [CURRENT_HOT_MEMOIZE_INDEX_KEY]: 1
    };
  }
  const index = _module.hot.data[CURRENT_HOT_MEMOIZE_INDEX_KEY]++;
  const value = _module.hot.data[index] ?? valueFactory();
  _module.hot.addDisposeHandler((data) => {
    data[index] = value;
  });
  return value;
}

const escapeRegExp = (text) => {
  return text.replace(/[.*+?^${}(\)|[\]\\]/g, "\\$&");
};

class WinstonLogger {
  #winston;
  #addRedactions;
  /**
   * Creates a {@link WinstonLogger} instance.
   */
  static create(options) {
    const redacter = WinstonLogger.redacter();
    const defaultFormatter = process.env.NODE_ENV === "production" ? winston.format.json() : WinstonLogger.colorFormat();
    let logger = winston.createLogger({
      level: process.env.LOG_LEVEL || options.level || "info",
      format: winston.format.combine(
        options.format ?? defaultFormatter,
        redacter.format
      ),
      transports: options.transports ?? new winston.transports.Console()
    });
    if (options.meta) {
      logger = logger.child(options.meta);
    }
    return new WinstonLogger(logger, redacter.add);
  }
  /**
   * Creates a winston log formatter for redacting secrets.
   */
  static redacter() {
    const redactionSet = /* @__PURE__ */ new Set();
    let redactionPattern = void 0;
    return {
      format: winston.format((obj) => {
        if (!redactionPattern || !obj) {
          return obj;
        }
        obj[tripleBeam.MESSAGE] = obj[tripleBeam.MESSAGE]?.replace?.(redactionPattern, "***");
        return obj;
      })(),
      add(newRedactions) {
        let added = 0;
        for (const redactionToTrim of newRedactions) {
          const redaction = redactionToTrim.trim();
          if (redaction.length <= 1) {
            continue;
          }
          if (!redactionSet.has(redaction)) {
            redactionSet.add(redaction);
            added += 1;
          }
        }
        if (added > 0) {
          const redactions = Array.from(redactionSet).map((r) => escapeRegExp(r)).join("|");
          redactionPattern = new RegExp(`(${redactions})`, "g");
        }
      }
    };
  }
  /**
   * Creates a pretty printed winston log formatter.
   */
  static colorFormat() {
    const colorizer = winston.format.colorize();
    return winston.format.combine(
      winston.format.timestamp(),
      winston.format.colorize({
        colors: {
          timestamp: "dim",
          prefix: "blue",
          field: "cyan",
          debug: "grey"
        }
      }),
      winston.format.printf((info) => {
        const { timestamp, level, message, plugin, service, ...fields } = info;
        const prefix = plugin || service;
        const timestampColor = colorizer.colorize("timestamp", timestamp);
        const prefixColor = colorizer.colorize("prefix", prefix);
        const extraFields = Object.entries(fields).map(
          ([key, value]) => `${colorizer.colorize("field", `${key}`)}=${value}`
        ).join(" ");
        return `${timestampColor} ${prefixColor} ${level} ${message} ${extraFields}`;
      })
    );
  }
  constructor(winston, addRedactions) {
    this.#winston = winston;
    this.#addRedactions = addRedactions;
  }
  error(message, meta) {
    this.#winston.error(message, meta);
  }
  warn(message, meta) {
    this.#winston.warn(message, meta);
  }
  info(message, meta) {
    this.#winston.info(message, meta);
  }
  debug(message, meta) {
    this.#winston.debug(message, meta);
  }
  child(meta) {
    return new WinstonLogger(this.#winston.child(meta));
  }
  addRedactions(redactions) {
    this.#addRedactions?.(redactions);
  }
}

function getVoidLogger() {
  return winston__namespace.createLogger({
    transports: [new winston__namespace.transports.Console({ silent: true })]
  });
}
let rootLogger;
function getRootLogger() {
  if (!rootLogger) {
    rootLogger = createRootLogger();
  }
  return rootLogger;
}
function setRootLogger(newLogger) {
  rootLogger = newLogger;
}

const getRedacter = /* @__PURE__ */ (() => {
  let redacter = void 0;
  return () => {
    if (!redacter) {
      redacter = WinstonLogger.redacter();
    }
    return redacter;
  };
})();
const setRootLoggerRedactionList = (redactions) => {
  getRedacter().add(redactions);
};
function redactWinstonLogLine(info) {
  return getRedacter().format.transform(
    info
  );
}
const colorizer = winston.format.colorize();
const coloredFormat = winston.format.combine(
  winston.format.timestamp(),
  winston.format.colorize({
    colors: {
      timestamp: "dim",
      prefix: "blue",
      field: "cyan",
      debug: "grey"
    }
  }),
  winston.format.printf((info) => {
    const { timestamp, level, message, plugin, service, ...fields } = info;
    const prefix = plugin || service;
    const timestampColor = colorizer.colorize("timestamp", timestamp);
    const prefixColor = colorizer.colorize("prefix", prefix);
    const extraFields = Object.entries(fields).map(
      ([key, value]) => `${colorizer.colorize("field", `${key}`)}=${value}`
    ).join(" ");
    return `${timestampColor} ${prefixColor} ${level} ${message} ${extraFields}`;
  })
);
function createRootLogger(options = {}, env = process.env) {
  const logger = winston__namespace.createLogger(
    lodash.merge(
      {
        level: env.LOG_LEVEL || "info",
        format: winston__namespace.format.combine(
          getRedacter().format,
          env.NODE_ENV === "production" ? winston__namespace.format.json() : WinstonLogger.colorFormat()
        ),
        transports: [
          new winston__namespace.transports.Console({
            silent: env.JEST_WORKER_ID !== void 0 && !env.LOG_LEVEL
          })
        ]
      },
      options
    )
  ).child({ service: "backstage" });
  setRootLogger(logger);
  return logger;
}

async function createConfigSecretEnumerator$1(options) {
  const { logger, dir = process.cwd() } = options;
  const { packages } = await getPackages.getPackages(dir);
  const schema = options.schema ?? await configLoader.loadConfigSchema({
    dependencies: packages.map((p) => p.packageJson.name)
  });
  return (config) => {
    const [secretsData] = schema.process(
      [{ data: config.getOptional() ?? {}, context: "schema-enumerator" }],
      {
        visibility: ["secret"],
        ignoreSchemaErrors: true
      }
    );
    const secrets = /* @__PURE__ */ new Set();
    JSON.parse(
      JSON.stringify(secretsData.data),
      (_, v) => typeof v === "string" && secrets.add(v)
    );
    logger.info(
      `Found ${secrets.size} new secrets in config that will be redacted`
    );
    return secrets;
  };
}

class ObservableConfigProxy {
  constructor(parent, parentKey) {
    this.parent = parent;
    this.parentKey = parentKey;
    if (parent && !parentKey) {
      throw new Error("parentKey is required if parent is set");
    }
  }
  config = new config.ConfigReader({});
  subscribers = [];
  setConfig(config) {
    if (this.parent) {
      throw new Error("immutable");
    }
    this.config = config;
    for (const subscriber of this.subscribers) {
      try {
        subscriber();
      } catch (error) {
        console.error(`Config subscriber threw error, ${error}`);
      }
    }
  }
  subscribe(onChange) {
    if (this.parent) {
      return this.parent.subscribe(onChange);
    }
    this.subscribers.push(onChange);
    return {
      unsubscribe: () => {
        const index = this.subscribers.indexOf(onChange);
        if (index >= 0) {
          this.subscribers.splice(index, 1);
        }
      }
    };
  }
  select(required) {
    if (this.parent && this.parentKey) {
      if (required) {
        return this.parent.select(true).getConfig(this.parentKey);
      }
      return this.parent.select(false)?.getOptionalConfig(this.parentKey);
    }
    return this.config;
  }
  has(key) {
    return this.select(false)?.has(key) ?? false;
  }
  keys() {
    return this.select(false)?.keys() ?? [];
  }
  get(key) {
    return this.select(true).get(key);
  }
  getOptional(key) {
    return this.select(false)?.getOptional(key);
  }
  getConfig(key) {
    return new ObservableConfigProxy(this, key);
  }
  getOptionalConfig(key) {
    if (this.select(false)?.has(key)) {
      return new ObservableConfigProxy(this, key);
    }
    return void 0;
  }
  getConfigArray(key) {
    return this.select(true).getConfigArray(key);
  }
  getOptionalConfigArray(key) {
    return this.select(false)?.getOptionalConfigArray(key);
  }
  getNumber(key) {
    return this.select(true).getNumber(key);
  }
  getOptionalNumber(key) {
    return this.select(false)?.getOptionalNumber(key);
  }
  getBoolean(key) {
    return this.select(true).getBoolean(key);
  }
  getOptionalBoolean(key) {
    return this.select(false)?.getOptionalBoolean(key);
  }
  getString(key) {
    return this.select(true).getString(key);
  }
  getOptionalString(key) {
    return this.select(false)?.getOptionalString(key);
  }
  getStringArray(key) {
    return this.select(true).getStringArray(key);
  }
  getOptionalStringArray(key) {
    return this.select(false)?.getOptionalStringArray(key);
  }
}

function isValidUrl(url) {
  try {
    new URL(url);
    return true;
  } catch {
    return false;
  }
}

const createConfigSecretEnumerator = createConfigSecretEnumerator$1;
async function loadBackendConfig(options) {
  const secretEnumerator = await createConfigSecretEnumerator({
    logger: options.logger
  });
  const { config } = await newLoadBackendConfig(options);
  setRootLoggerRedactionList(secretEnumerator(config));
  config.subscribe?.(
    () => setRootLoggerRedactionList(secretEnumerator(config))
  );
  return config;
}
async function newLoadBackendConfig(options) {
  const args = parseArgs__default.default(options.argv);
  const configTargets = [args.config ?? []].flat().map((arg) => isValidUrl(arg) ? { url: arg } : { path: path.resolve(arg) });
  const paths = cliCommon.findPaths(__dirname);
  let currentCancelFunc = void 0;
  const config$1 = new ObservableConfigProxy();
  const { appConfigs } = await configLoader.loadConfig({
    configRoot: paths.targetRoot,
    configTargets,
    remote: options.remote,
    watch: options.watch ?? true ? {
      onChange(newConfigs) {
        console.info(
          `Reloaded config from ${newConfigs.map((c) => c.context).join(", ")}`
        );
        const configsToMerge = [...newConfigs];
        if (options.additionalConfigs) {
          configsToMerge.push(...options.additionalConfigs);
        }
        config$1.setConfig(config.ConfigReader.fromConfigs(configsToMerge));
      },
      stopSignal: new Promise((resolve) => {
        if (currentCancelFunc) {
          currentCancelFunc();
        }
        currentCancelFunc = resolve;
        if (module.hot) {
          module.hot.addDisposeHandler(resolve);
        }
      })
    } : void 0
  });
  console.info(
    `Loaded config from ${appConfigs.map((c) => c.context).join(", ")}`
  );
  const finalAppConfigs = [...appConfigs];
  if (options.additionalConfigs) {
    finalAppConfigs.push(...options.additionalConfigs);
  }
  config$1.setConfig(config.ConfigReader.fromConfigs(finalAppConfigs));
  return { config: config$1 };
}

function isAuthCallbackOptions(options) {
  return "onAuth" in options;
}
class Git {
  constructor(config) {
    this.config = config;
    this.onAuth = config.onAuth;
    this.headers = {
      "user-agent": "git/@isomorphic-git",
      ...config.token ? { Authorization: `Bearer ${config.token}` } : {}
    };
  }
  headers;
  async add(options) {
    const { dir, filepath } = options;
    this.config.logger?.info(`Adding file {dir=${dir},filepath=${filepath}}`);
    return git__default.default.add({ fs: fs__default.default, dir, filepath });
  }
  async addRemote(options) {
    const { dir, url, remote, force } = options;
    this.config.logger?.info(
      `Creating new remote {dir=${dir},remote=${remote},url=${url}}`
    );
    return git__default.default.addRemote({ fs: fs__default.default, dir, remote, url, force });
  }
  async deleteRemote(options) {
    const { dir, remote } = options;
    this.config.logger?.info(`Deleting remote {dir=${dir},remote=${remote}}`);
    return git__default.default.deleteRemote({ fs: fs__default.default, dir, remote });
  }
  async checkout(options) {
    const { dir, ref } = options;
    this.config.logger?.info(`Checking out branch {dir=${dir},ref=${ref}}`);
    return git__default.default.checkout({ fs: fs__default.default, dir, ref });
  }
  async branch(options) {
    const { dir, ref } = options;
    this.config.logger?.info(`Creating branch {dir=${dir},ref=${ref}`);
    return git__default.default.branch({ fs: fs__default.default, dir, ref });
  }
  async commit(options) {
    const { dir, message, author, committer } = options;
    this.config.logger?.info(
      `Committing file to repo {dir=${dir},message=${message}}`
    );
    return git__default.default.commit({ fs: fs__default.default, dir, message, author, committer });
  }
  /** https://isomorphic-git.org/docs/en/clone */
  async clone(options) {
    const { url, dir, ref, depth, noCheckout } = options;
    this.config.logger?.info(`Cloning repo {dir=${dir},url=${url}}`);
    try {
      return await git__default.default.clone({
        fs: fs__default.default,
        http: http__default.default,
        url,
        dir,
        ref,
        singleBranch: true,
        depth: depth ?? 1,
        noCheckout,
        onProgress: this.onProgressHandler(),
        headers: this.headers,
        onAuth: this.onAuth
      });
    } catch (ex) {
      this.config.logger?.error(`Failed to clone repo {dir=${dir},url=${url}}`);
      if (ex.data) {
        throw new Error(`${ex.message} {data=${JSON.stringify(ex.data)}}`);
      }
      throw ex;
    }
  }
  /** https://isomorphic-git.org/docs/en/currentBranch */
  async currentBranch(options) {
    const { dir, fullName = false } = options;
    return git__default.default.currentBranch({ fs: fs__default.default, dir, fullname: fullName });
  }
  /** https://isomorphic-git.org/docs/en/fetch */
  async fetch(options) {
    const { dir, remote = "origin", tags = false } = options;
    this.config.logger?.info(
      `Fetching remote=${remote} for repository {dir=${dir}}`
    );
    try {
      await git__default.default.fetch({
        fs: fs__default.default,
        http: http__default.default,
        dir,
        remote,
        tags,
        onProgress: this.onProgressHandler(),
        headers: this.headers,
        onAuth: this.onAuth
      });
    } catch (ex) {
      this.config.logger?.error(
        `Failed to fetch repo {dir=${dir},remote=${remote}}`
      );
      if (ex.data) {
        throw new Error(`${ex.message} {data=${JSON.stringify(ex.data)}}`);
      }
      throw ex;
    }
  }
  async init(options) {
    const { dir, defaultBranch = "master" } = options;
    this.config.logger?.info(`Init git repository {dir=${dir}}`);
    return git__default.default.init({
      fs: fs__default.default,
      dir,
      defaultBranch
    });
  }
  /** https://isomorphic-git.org/docs/en/merge */
  async merge(options) {
    const { dir, theirs, ours, author, committer } = options;
    this.config.logger?.info(
      `Merging branch '${theirs}' into '${ours}' for repository {dir=${dir}}`
    );
    return git__default.default.merge({
      fs: fs__default.default,
      dir,
      ours,
      theirs,
      author,
      committer
    });
  }
  async push(options) {
    const { dir, remote, remoteRef, force } = options;
    this.config.logger?.info(
      `Pushing directory to remote {dir=${dir},remote=${remote}}`
    );
    try {
      return await git__default.default.push({
        fs: fs__default.default,
        dir,
        http: http__default.default,
        onProgress: this.onProgressHandler(),
        remoteRef,
        force,
        headers: this.headers,
        remote,
        onAuth: this.onAuth
      });
    } catch (ex) {
      this.config.logger?.error(
        `Failed to push to repo {dir=${dir}, remote=${remote}}`
      );
      if (ex.data) {
        throw new Error(`${ex.message} {data=${JSON.stringify(ex.data)}}`);
      }
      throw ex;
    }
  }
  /** https://isomorphic-git.org/docs/en/readCommit */
  async readCommit(options) {
    const { dir, sha } = options;
    return git__default.default.readCommit({ fs: fs__default.default, dir, oid: sha });
  }
  /** https://isomorphic-git.org/docs/en/remove */
  async remove(options) {
    const { dir, filepath } = options;
    this.config.logger?.info(
      `Removing file from git index {dir=${dir},filepath=${filepath}}`
    );
    return git__default.default.remove({ fs: fs__default.default, dir, filepath });
  }
  /** https://isomorphic-git.org/docs/en/resolveRef */
  async resolveRef(options) {
    const { dir, ref } = options;
    return git__default.default.resolveRef({ fs: fs__default.default, dir, ref });
  }
  /** https://isomorphic-git.org/docs/en/log */
  async log(options) {
    const { dir, ref } = options;
    return git__default.default.log({
      fs: fs__default.default,
      dir,
      ref: ref ?? "HEAD"
    });
  }
  onAuth;
  onProgressHandler = () => {
    let currentPhase = "";
    return (event) => {
      if (currentPhase !== event.phase) {
        currentPhase = event.phase;
        this.config.logger?.info(event.phase);
      }
      const total = event.total ? `${Math.round(event.loaded / event.total * 100)}%` : event.loaded;
      this.config.logger?.debug(`status={${event.phase},total={${total}}}`);
    };
  };
  static fromAuth = (options) => {
    if (isAuthCallbackOptions(options)) {
      const { onAuth, logger: logger2 } = options;
      return new Git({ onAuth, logger: logger2 });
    }
    const { username, password, token, logger } = options;
    return new Git({ onAuth: () => ({ username, password }), token, logger });
  };
}

const TOKEN_ALG = "HS256";
const TOKEN_SUB = "backstage-server";
const TOKEN_EXPIRY_AFTER = luxon.Duration.fromObject({ hours: 1 });
const TOKEN_REISSUE_AFTER = luxon.Duration.fromObject({ minutes: 10 });
class NoopTokenManager {
  isInsecureServerTokenManager = true;
  async getToken() {
    return { token: "" };
  }
  async authenticate() {
  }
}
class DisabledTokenManager {
  async getToken() {
    throw new Error(
      "Unable to generate legacy token, no legacy keys are configured in 'backend.auth.keys' or 'backend.auth.externalAccess'"
    );
  }
  async authenticate() {
    throw new errors.AuthenticationError(
      "Unable to authenticate legacy token, no legacy keys are configured in 'backend.auth.keys' or 'backend.auth.externalAccess'"
    );
  }
}
class ServerTokenManager {
  options;
  verificationKeys;
  signingKey;
  privateKeyPromise;
  currentTokenPromise;
  /**
   * Creates a token manager that issues static fake tokens and never fails
   * authentication. This can be useful for testing.
   */
  static noop() {
    return new NoopTokenManager();
  }
  static fromConfig(config, options) {
    const oldSecrets = config.getOptionalConfigArray("backend.auth.keys")?.map((c) => c.getString("secret"));
    const newSecrets = config.getOptionalConfigArray("backend.auth.externalAccess")?.filter((c) => c.getString("type") === "legacy").map((c) => c.getString("options.secret"));
    const secrets = [...oldSecrets ?? [], ...newSecrets ?? []];
    if (secrets.length) {
      return new ServerTokenManager(secrets, options);
    }
    if (options.allowDisabledTokenManager) {
      return new DisabledTokenManager();
    }
    if (process.env.NODE_ENV !== "development") {
      throw new Error(
        "You must configure at least one key in backend.auth.keys for production."
      );
    }
    options.logger.warn(
      "Generated a secret for service-to-service authentication: DEVELOPMENT USE ONLY."
    );
    return new ServerTokenManager([], options);
  }
  constructor(secrets, options) {
    if (!secrets.length && process.env.NODE_ENV !== "development") {
      throw new Error(
        "No secrets provided when constructing ServerTokenManager"
      );
    }
    this.options = options;
    this.verificationKeys = secrets.map((s) => jose.base64url.decode(s));
    this.signingKey = this.verificationKeys[0];
  }
  // Called when no keys have been generated yet in the dev environment
  async generateKeys() {
    if (process.env.NODE_ENV !== "development") {
      throw new Error(
        "Key generation is not supported outside of the dev environment"
      );
    }
    if (this.privateKeyPromise) {
      return this.privateKeyPromise;
    }
    const promise = (async () => {
      const secret = await jose.generateSecret(TOKEN_ALG);
      const jwk = await jose.exportJWK(secret);
      this.verificationKeys.push(jose.base64url.decode(jwk.k ?? ""));
      this.signingKey = this.verificationKeys[0];
      return;
    })();
    try {
      this.privateKeyPromise = promise;
      await promise;
    } catch (error) {
      this.options.logger.error(`Failed to generate new key, ${error}`);
      delete this.privateKeyPromise;
    }
    return promise;
  }
  async getToken() {
    if (!this.verificationKeys.length) {
      await this.generateKeys();
    }
    if (this.currentTokenPromise) {
      return this.currentTokenPromise;
    }
    const result = Promise.resolve().then(async () => {
      const jwt = await new jose.SignJWT({}).setProtectedHeader({ alg: TOKEN_ALG }).setSubject(TOKEN_SUB).setExpirationTime(
        luxon.DateTime.now().plus(TOKEN_EXPIRY_AFTER).toUnixInteger()
      ).sign(this.signingKey);
      return { token: jwt };
    });
    this.currentTokenPromise = result;
    result.then(() => {
      setTimeout(() => {
        this.currentTokenPromise = void 0;
      }, TOKEN_REISSUE_AFTER.toMillis());
    }).catch(() => {
      this.currentTokenPromise = void 0;
    });
    return result;
  }
  async authenticate(token) {
    let verifyError = void 0;
    for (const key of this.verificationKeys) {
      try {
        const {
          protectedHeader: { alg },
          payload: { sub, exp }
        } = await jose.jwtVerify(token, key);
        if (alg !== TOKEN_ALG) {
          throw new errors.AuthenticationError(`Illegal alg "${alg}"`);
        }
        if (sub !== TOKEN_SUB) {
          throw new errors.AuthenticationError(`Illegal sub "${sub}"`);
        }
        if (typeof exp !== "number") {
          throw new errors.AuthenticationError(
            "Server-to-server token had no exp claim"
          );
        }
        return;
      } catch (e) {
        verifyError = e;
      }
    }
    throw new errors.AuthenticationError("Invalid server token", verifyError);
  }
}

function readHelmetOptions(config) {
  const cspOptions = readCspDirectives(config);
  return {
    contentSecurityPolicy: {
      useDefaults: false,
      directives: applyCspDirectives$1(cspOptions)
    },
    // These are all disabled in order to maintain backwards compatibility
    // when bumping helmet v5. We can't enable these by default because
    // there is no way for users to configure them.
    // TODO(Rugvip): We should give control of this setup to consumers
    crossOriginEmbedderPolicy: false,
    crossOriginOpenerPolicy: false,
    crossOriginResourcePolicy: false,
    originAgentCluster: false
  };
}
function readCspDirectives(config) {
  const cc = config?.getOptionalConfig("csp");
  if (!cc) {
    return void 0;
  }
  const result = {};
  for (const key of cc.keys()) {
    if (cc.get(key) === false) {
      result[key] = false;
    } else {
      result[key] = cc.getStringArray(key);
    }
  }
  return result;
}
function applyCspDirectives$1(directives) {
  const result = helmet__default.default.contentSecurityPolicy.getDefaultDirectives();
  result["script-src"] = ["'self'", "'unsafe-eval'"];
  delete result["form-action"];
  if (directives) {
    for (const [key, value] of Object.entries(directives)) {
      const kebabCaseKey = kebabCase__default.default(key);
      if (value === false) {
        delete result[kebabCaseKey];
      } else {
        result[kebabCaseKey] = value;
      }
    }
  }
  return result;
}

function readCorsOptions(config) {
  const cc = config?.getOptionalConfig("cors");
  if (!cc) {
    return { origin: false };
  }
  return removeUnknown({
    origin: createCorsOriginMatcher(readStringArray(cc, "origin")),
    methods: readStringArray(cc, "methods"),
    allowedHeaders: readStringArray(cc, "allowedHeaders"),
    exposedHeaders: readStringArray(cc, "exposedHeaders"),
    credentials: cc.getOptionalBoolean("credentials"),
    maxAge: cc.getOptionalNumber("maxAge"),
    preflightContinue: cc.getOptionalBoolean("preflightContinue"),
    optionsSuccessStatus: cc.getOptionalNumber("optionsSuccessStatus")
  });
}
function removeUnknown(obj) {
  return Object.fromEntries(
    Object.entries(obj).filter(([, v]) => v !== void 0)
  );
}
function readStringArray(config, key) {
  const value = config.getOptional(key);
  if (typeof value === "string") {
    return [value];
  } else if (!value) {
    return void 0;
  }
  return config.getStringArray(key);
}
function createCorsOriginMatcher(allowedOriginPatterns) {
  if (!allowedOriginPatterns) {
    return void 0;
  }
  const allowedOriginMatchers = allowedOriginPatterns.map(
    (pattern) => new minimatch.Minimatch(pattern, { nocase: true, noglobstar: true })
  );
  return (origin, callback) => {
    return callback(
      null,
      allowedOriginMatchers.some((pattern) => pattern.match(origin ?? ""))
    );
  };
}

function handleBadError(error, logger) {
  const logId = crypto.randomBytes(10).toString("hex");
  logger.child({ logId }).error(`Filtered internal error with logId=${logId} from response`, error);
  const newError = new Error(`An internal error occurred logId=${logId}`);
  delete newError.stack;
  return newError;
}
function applyInternalErrorFilter(error, logger) {
  try {
    errors.assertError(error);
  } catch (assertionError) {
    errors.assertError(assertionError);
    return handleBadError(assertionError, logger);
  }
  const constructorName = error.constructor.name;
  if (constructorName === "DatabaseError") {
    return handleBadError(error, logger);
  }
  return error;
}

class MiddlewareFactory {
  #config;
  #logger;
  /**
   * Creates a new {@link MiddlewareFactory}.
   */
  static create(options) {
    return new MiddlewareFactory(options);
  }
  constructor(options) {
    this.#config = options.config;
    this.#logger = options.logger;
  }
  /**
   * Returns a middleware that unconditionally produces a 404 error response.
   *
   * @remarks
   *
   * Typically you want to place this middleware at the end of the chain, such
   * that it's the last one attempted after no other routes matched.
   *
   * @returns An Express request handler
   */
  notFound() {
    return (_req, res) => {
      res.status(404).end();
    };
  }
  /**
   * Returns the compression middleware.
   *
   * @remarks
   *
   * The middleware will attempt to compress response bodies for all requests
   * that traverse through the middleware.
   */
  compression() {
    return compression__default.default();
  }
  /**
   * Returns a request logging middleware.
   *
   * @remarks
   *
   * Typically you want to place this middleware at the start of the chain, such
   * that it always logs requests whether they are "caught" by handlers farther
   * down or not.
   *
   * @returns An Express request handler
   */
  logging() {
    const logger = this.#logger.child({
      type: "incomingRequest"
    });
    const customMorganFormat = '[:date[clf]] ":method :url HTTP/:http-version" :status :res[content-length] ":referrer" ":user-agent"';
    return morgan__default.default(customMorganFormat, {
      stream: {
        write(message) {
          logger.info(message.trimEnd());
        }
      }
    });
  }
  /**
   * Returns a middleware that implements the helmet library.
   *
   * @remarks
   *
   * This middleware applies security policies to incoming requests and outgoing
   * responses. It is configured using config keys such as `backend.csp`.
   *
   * @see {@link https://helmetjs.github.io/}
   *
   * @returns An Express request handler
   */
  helmet() {
    return helmet__default.default(readHelmetOptions(this.#config.getOptionalConfig("backend")));
  }
  /**
   * Returns a middleware that implements the cors library.
   *
   * @remarks
   *
   * This middleware handles CORS. It is configured using the config key
   * `backend.cors`.
   *
   * @see {@link https://github.com/expressjs/cors}
   *
   * @returns An Express request handler
   */
  cors() {
    return cors__default.default(readCorsOptions(this.#config.getOptionalConfig("backend")));
  }
  /**
   * Express middleware to handle errors during request processing.
   *
   * @remarks
   *
   * This is commonly the very last middleware in the chain.
   *
   * Its primary purpose is not to do translation of business logic exceptions,
   * but rather to be a global catch-all for uncaught "fatal" errors that are
   * expected to result in a 500 error. However, it also does handle some common
   * error types (such as http-error exceptions, and the well-known error types
   * in the `@backstage/errors` package) and returns the enclosed status code
   * accordingly.
   *
   * It will also produce a response body with a serialized form of the error,
   * unless a previous handler already did send a body. See
   * {@link @backstage/errors#ErrorResponseBody} for the response shape used.
   *
   * @returns An Express error request handler
   */
  error(options = {}) {
    const showStackTraces = options.showStackTraces ?? process.env.NODE_ENV === "development";
    const logger = this.#logger.child({
      type: "errorHandler"
    });
    return (rawError, req, res, next) => {
      const error = applyInternalErrorFilter(rawError, logger);
      const statusCode = getStatusCode(error);
      if (options.logAllErrors || statusCode >= 500) {
        logger.error(`Request failed with status ${statusCode}`, error);
      }
      if (res.headersSent) {
        next(error);
        return;
      }
      const body = {
        error: errors.serializeError(error, { includeStack: showStackTraces }),
        request: { method: req.method, url: req.url },
        response: { statusCode }
      };
      res.status(statusCode).json(body);
    };
  }
}
function getStatusCode(error) {
  const knownStatusCodeFields = ["statusCode", "status"];
  for (const field of knownStatusCodeFields) {
    const statusCode = error[field];
    if (typeof statusCode === "number" && (statusCode | 0) === statusCode && // is whole integer
    statusCode >= 100 && statusCode <= 599) {
      return statusCode;
    }
  }
  switch (error.name) {
    case errors.NotModifiedError.name:
      return 304;
    case errors.InputError.name:
      return 400;
    case errors.AuthenticationError.name:
      return 401;
    case errors.NotAllowedError.name:
      return 403;
    case errors.NotFoundError.name:
      return 404;
    case errors.ConflictError.name:
      return 409;
    case errors.NotImplementedError.name:
      return 501;
    case errors.ServiceUnavailableError.name:
      return 503;
  }
  return 500;
}

function errorHandler(options = {}) {
  return MiddlewareFactory.create({
    config: new config.ConfigReader({}),
    logger: options.logger ?? getRootLogger()
  }).error({
    logAllErrors: options.logClientErrors,
    showStackTraces: options.showStackTraces
  });
}

function notFoundHandler() {
  return MiddlewareFactory.create({
    config: new config.ConfigReader({}),
    logger: getRootLogger()
  }).notFound();
}

function requestLoggingHandler(logger) {
  return MiddlewareFactory.create({
    config: new config.ConfigReader({}),
    logger: logger ?? getRootLogger()
  }).logging();
}

async function statusCheckHandler(options = {}) {
  const statusCheck = options.statusCheck ? options.statusCheck : () => Promise.resolve({ status: "ok" });
  return async (_request, response, next) => {
    try {
      const status = await statusCheck();
      response.status(200).json(status);
    } catch (err) {
      next(err);
    }
  };
}

const FIVE_DAYS_IN_MS = 5 * 24 * 60 * 60 * 1e3;
const IP_HOSTNAME_REGEX = /:|^\d+\.\d+\.\d+\.\d+$/;
async function getGeneratedCertificate(hostname, logger) {
  const hasModules = await fs__default.default.pathExists("node_modules");
  let certPath;
  if (hasModules) {
    certPath = path.resolve(
      "node_modules/.cache/backstage-backend/dev-cert.pem"
    );
    await fs__default.default.ensureDir(path.dirname(certPath));
  } else {
    certPath = path.resolve(".dev-cert.pem");
  }
  if (await fs__default.default.pathExists(certPath)) {
    try {
      const cert = await fs__default.default.readFile(certPath);
      const crt = forge__default.default.pki.certificateFromPem(cert.toString());
      const remainingMs = crt.validity.notAfter.getTime() - Date.now();
      if (remainingMs > FIVE_DAYS_IN_MS) {
        logger.info("Using existing self-signed certificate");
        return {
          key: cert,
          cert
        };
      }
    } catch (error) {
      logger.warn(`Unable to use existing self-signed certificate, ${error}`);
    }
  }
  logger.info("Generating new self-signed certificate");
  const newCert = await generateCertificate(hostname);
  await fs__default.default.writeFile(certPath, newCert.cert + newCert.key, "utf8");
  return newCert;
}
async function generateCertificate(hostname) {
  const attributes = [
    {
      name: "commonName",
      value: "dev-cert"
    }
  ];
  const sans = [
    {
      type: 2,
      // DNS
      value: "localhost"
    },
    {
      type: 2,
      value: "localhost.localdomain"
    },
    {
      type: 2,
      value: "[::1]"
    },
    {
      type: 7,
      // IP
      ip: "127.0.0.1"
    },
    {
      type: 7,
      ip: "fe80::1"
    }
  ];
  if (!sans.find(({ value, ip }) => value === hostname || ip === hostname)) {
    sans.push(
      IP_HOSTNAME_REGEX.test(hostname) ? {
        type: 7,
        ip: hostname
      } : {
        type: 2,
        value: hostname
      }
    );
  }
  const params = {
    algorithm: "sha256",
    keySize: 2048,
    days: 30,
    extensions: [
      {
        name: "keyUsage",
        keyCertSign: true,
        digitalSignature: true,
        nonRepudiation: true,
        keyEncipherment: true,
        dataEncipherment: true
      },
      {
        name: "extKeyUsage",
        serverAuth: true,
        clientAuth: true,
        codeSigning: true,
        timeStamping: true
      },
      {
        name: "subjectAltName",
        altNames: sans
      }
    ]
  };
  return new Promise(
    (resolve, reject) => require("selfsigned").generate(
      attributes,
      params,
      (err, bundle) => {
        if (err) {
          reject(err);
        } else {
          resolve({ key: bundle.private, cert: bundle.cert });
        }
      }
    )
  );
}

async function createHttpServer(listener, options, deps) {
  const server = await createServer(listener, options, deps);
  const stopper = stoppableServer__default.default(server, 0);
  const stopServer = stopper.stop.bind(stopper);
  return Object.assign(server, {
    start() {
      return new Promise((resolve, reject) => {
        const handleStartupError = (error) => {
          server.close();
          reject(error);
        };
        server.on("error", handleStartupError);
        const { host, port } = options.listen;
        server.listen(port, host, () => {
          server.off("error", handleStartupError);
          deps.logger.info(`Listening on ${host}:${port}`);
          resolve();
        });
      });
    },
    stop() {
      return new Promise((resolve, reject) => {
        stopServer((error) => {
          if (error) {
            reject(error);
          } else {
            resolve();
          }
        });
      });
    },
    port() {
      const address = server.address();
      if (typeof address === "string" || address === null) {
        throw new Error(`Unexpected server address '${address}'`);
      }
      return address.port;
    }
  });
}
async function createServer(listener, options, deps) {
  if (options.https) {
    const { certificate } = options.https;
    if (certificate.type === "generated") {
      const credentials = await getGeneratedCertificate(
        certificate.hostname,
        deps.logger
      );
      return https__namespace.createServer(credentials, listener);
    }
    return https__namespace.createServer(certificate, listener);
  }
  return http__namespace.createServer(listener);
}

class ServiceBuilderImpl {
  logger;
  serverOptions;
  helmetOptions;
  corsOptions;
  routers;
  requestLoggingHandler;
  errorHandler;
  useDefaultErrorHandler;
  // Reference to the module where builder is created - needed for hot module
  // reloading
  module;
  constructor(moduleRef) {
    this.routers = [];
    this.module = moduleRef;
    this.useDefaultErrorHandler = true;
    this.serverOptions = readHttpServerOptions();
    this.corsOptions = readCorsOptions();
    this.helmetOptions = readHelmetOptions();
  }
  loadConfig(config) {
    const backendConfig = config.getOptionalConfig("backend");
    this.serverOptions = readHttpServerOptions(backendConfig);
    this.corsOptions = readCorsOptions(backendConfig);
    this.helmetOptions = readHelmetOptions(backendConfig);
    return this;
  }
  setPort(port) {
    this.serverOptions.listen.port = port;
    return this;
  }
  setHost(host) {
    this.serverOptions.listen.host = host;
    return this;
  }
  setLogger(logger) {
    this.logger = logger;
    return this;
  }
  setHttpsSettings(settings) {
    if ("hostname" in settings.certificate) {
      this.serverOptions.https = {
        certificate: {
          ...settings.certificate,
          type: "generated"
        }
      };
    } else {
      this.serverOptions.https = {
        certificate: {
          ...settings.certificate,
          type: "pem"
        }
      };
    }
    return this;
  }
  enableCors(options) {
    this.corsOptions = options;
    return this;
  }
  setCsp(options) {
    const csp = this.helmetOptions.contentSecurityPolicy;
    this.helmetOptions.contentSecurityPolicy = {
      ...typeof csp === "object" ? csp : {},
      directives: applyCspDirectives(options)
    };
    return this;
  }
  addRouter(root, router) {
    this.routers.push([root, router]);
    return this;
  }
  setRequestLoggingHandler(requestLoggingHandler) {
    this.requestLoggingHandler = requestLoggingHandler;
    return this;
  }
  setErrorHandler(errorHandler) {
    this.errorHandler = errorHandler;
    return this;
  }
  disableDefaultErrorHandler() {
    this.useDefaultErrorHandler = false;
    return this;
  }
  async start() {
    const app = express__default.default();
    const logger = this.logger ?? getRootLogger();
    app.use(helmet__default.default(this.helmetOptions));
    app.use(cors__default.default(this.corsOptions));
    app.use(compression__default.default());
    app.use(
      (this.requestLoggingHandler ?? requestLoggingHandler)(logger)
    );
    for (const [root, route] of this.routers) {
      app.use(root, route);
    }
    app.use(notFoundHandler());
    if (this.errorHandler) {
      app.use(this.errorHandler);
    }
    if (this.useDefaultErrorHandler) {
      app.use(errorHandler());
    }
    const server = await createHttpServer(app, this.serverOptions, { logger });
    useHotCleanup(
      this.module,
      () => server.stop().catch((error) => {
        console.error(error);
      })
    );
    await server.start();
    return server;
  }
}
function applyCspDirectives(directives) {
  const result = helmet__default.default.contentSecurityPolicy.getDefaultDirectives();
  result["script-src"] = ["'self'", "'unsafe-eval'"];
  delete result["form-action"];
  if (directives) {
    for (const [key, value] of Object.entries(directives)) {
      if (value === false) {
        delete result[key];
      } else {
        result[key] = value;
      }
    }
  }
  return result;
}

function createServiceBuilder(_module) {
  return new ServiceBuilderImpl(_module);
}

async function createStatusCheckRouter(options) {
  const router = Router__default.default();
  const { path = "/healthcheck", statusCheck } = options;
  router.use(path, await statusCheckHandler({ statusCheck }));
  router.use(errorHandler());
  return router;
}

class DockerContainerRunner {
  dockerClient;
  constructor(options) {
    this.dockerClient = options.dockerClient;
  }
  async runContainer(options) {
    const {
      imageName,
      command,
      args,
      logStream = new stream.PassThrough(),
      mountDirs = {},
      workingDir,
      envVars = {},
      pullImage = true,
      defaultUser = false,
      pullOptions = {}
    } = options;
    try {
      await this.dockerClient.ping();
    } catch (e) {
      throw new errors.ForwardedError(
        "This operation requires Docker. Docker does not appear to be available. Docker.ping() failed with",
        e
      );
    }
    if (pullImage) {
      await new Promise((resolve, reject) => {
        this.dockerClient.pull(imageName, pullOptions, (err, stream) => {
          if (err) {
            reject(err);
          } else if (!stream) {
            reject(
              new Error(
                "Unexpeected error: no stream returned from Docker while pulling image"
              )
            );
          } else {
            stream.pipe(logStream, { end: false });
            stream.on("end", () => resolve());
            stream.on("error", (error2) => reject(error2));
          }
        });
      });
    }
    const userOptions = {};
    if (!defaultUser && process.getuid && process.getgid) {
      userOptions.User = `${process.getuid()}:${process.getgid()}`;
    }
    const Volumes = {};
    for (const containerDir of Object.values(mountDirs)) {
      Volumes[containerDir] = {};
    }
    const Binds = [];
    for (const [hostDir, containerDir] of Object.entries(mountDirs)) {
      const realHostDir = await fs__default.default.realpath(hostDir);
      Binds.push(`${realHostDir}:${containerDir}`);
    }
    const Env = new Array();
    for (const [key, value] of Object.entries(envVars)) {
      Env.push(`${key}=${value}`);
    }
    const [{ Error: error, StatusCode: statusCode }] = await this.dockerClient.run(imageName, args, logStream, {
      Volumes,
      HostConfig: {
        AutoRemove: true,
        Binds
      },
      ...workingDir ? { WorkingDir: workingDir } : {},
      Entrypoint: command,
      Env,
      ...userOptions
    });
    if (error) {
      throw new Error(
        `Docker failed to run with the following error message: ${error}`
      );
    }
    if (statusCode !== 0) {
      throw new Error(
        `Docker container returned a non-zero exit code (${statusCode})`
      );
    }
  }
}

class KubernetesContainerRunner {
  kubeConfig;
  batchV1Api;
  log;
  name;
  namespace;
  mountBase;
  podTemplate;
  timeoutMs;
  containerName = "executor";
  getNamespace(kubeConfig, namespace) {
    let _namespace = namespace;
    if (!_namespace) {
      _namespace = kubeConfig.getContextObject(
        kubeConfig.currentContext
      )?.namespace;
    }
    if (!_namespace) {
      throw new Error("Cannot read current namespace from Kubernetes cluster");
    }
    return _namespace;
  }
  validateMountBase(mountBase, podTemplate) {
    if (!podTemplate?.spec?.volumes?.filter((v) => v.name === mountBase.volumeName).length) {
      throw new Error(
        `A Pod template containing the volume ${mountBase.volumeName} is required`
      );
    }
    if (!mountBase.basePath.endsWith("/")) {
      mountBase.basePath += "/";
    }
    return mountBase;
  }
  constructor(options) {
    const { kubeConfig, name, namespace, mountBase, podTemplate, timeoutMs } = options;
    this.kubeConfig = kubeConfig;
    this.batchV1Api = kubeConfig.makeApiClient(clientNode.BatchV1Api);
    this.log = new clientNode.Log(kubeConfig);
    this.name = name;
    this.namespace = this.getNamespace(kubeConfig, namespace);
    if (mountBase) {
      this.mountBase = this.validateMountBase(mountBase, podTemplate);
    }
    this.podTemplate = podTemplate;
    this.timeoutMs = timeoutMs || 120 * 1e3;
  }
  async runContainer(options) {
    const {
      imageName,
      command,
      args,
      logStream,
      mountDirs = {},
      workingDir,
      envVars = {}
    } = options;
    const containerLogStream = new stream.PassThrough();
    if (logStream) {
      containerLogStream.pipe(logStream, { end: false });
    }
    const commandArr = typeof command === "string" ? [command] : command;
    const volumeMounts = [];
    for (const [hostDir, containerDir] of Object.entries(mountDirs)) {
      if (!this.mountBase) {
        throw new Error(
          "A volumeName and a basePath must be configured to bind mount directories"
        );
      }
      if (!hostDir.startsWith(this.mountBase.basePath)) {
        throw new Error(
          `Mounted '${hostDir}' dir should be subdirectories of '${this.mountBase.basePath}'`
        );
      }
      volumeMounts.push({
        name: this.mountBase.volumeName,
        mountPath: containerDir,
        subPath: hostDir.slice(this.mountBase.basePath.length)
      });
    }
    const env = [];
    for (const [key, value] of Object.entries(envVars)) {
      env.push({
        name: key,
        value
      });
    }
    const taskId = uuid.v4();
    const mergedPodTemplate = {
      metadata: {
        ...{
          labels: {
            task: taskId
          }
        },
        ...this.podTemplate?.metadata
      },
      spec: {
        ...{
          containers: [
            {
              name: this.containerName,
              image: imageName,
              command: commandArr,
              args,
              env,
              workingDir,
              volumeMounts
            }
          ],
          restartPolicy: "Never"
        },
        ...this.podTemplate?.spec
      }
    };
    const jobSpec = {
      metadata: {
        generateName: `${this.name}-`
      },
      spec: {
        backoffLimit: 0,
        ttlSecondsAfterFinished: 60,
        template: mergedPodTemplate
      }
    };
    await this.runJob(jobSpec, taskId, containerLogStream);
  }
  handleError(err, errorCallback) {
    if (err.code !== "ECONNRESET" && err.message !== "aborted") {
      errorCallback(
        handleKubernetesError(
          "Kubernetes watch request failed with the following error message:",
          err
        )
      );
    }
  }
  watchPod(taskId, callback, errorCallback) {
    const watch = new clientNode.Watch(this.kubeConfig);
    const labelSelector = `task=${taskId}`;
    return watch.watch(
      `/api/v1/namespaces/${this.namespace}/pods`,
      {
        labelSelector
      },
      (_, pod) => {
        callback(pod);
      },
      (err) => {
        if (err) {
          this.handleError(err, errorCallback);
        }
      }
    );
  }
  tailLogs(taskId, logStream) {
    let log;
    let req;
    const watchPromise = new Promise((_, reject) => {
      req = this.watchPod(
        taskId,
        (pod) => {
          if (log === void 0 && (pod.status?.phase === "Running" || pod.status?.phase === "Succeeded" || pod.status?.phase === "Failed")) {
            log = this.log.log(
              this.namespace,
              pod.metadata?.name,
              this.containerName,
              logStream,
              { follow: true }
            );
          }
        },
        reject
      );
    });
    const logPromise = new Promise((resolve, _) => {
      if (!logStream.writableFinished) {
        logStream.on("finish", () => {
          resolve();
        });
      } else {
        resolve();
      }
    });
    const close = async () => {
      if (req) {
        (await req).abort();
      }
      if (log) {
        (await log).abort();
      }
    };
    return { promise: Promise.race([watchPromise, logPromise]), close };
  }
  waitPod(taskId) {
    let req;
    const promise = new Promise(async (resolve, reject) => {
      req = this.watchPod(
        taskId,
        (pod) => {
          if (pod.status?.phase === "Succeeded") {
            resolve();
          }
          if (pod.status?.phase === "Failed") {
            reject(new Error("Container execution failed"));
          }
        },
        reject
      );
    });
    const close = async () => {
      if (req) {
        (await req).abort();
      }
    };
    return { promise, close };
  }
  async createJob(jobSpec) {
    return this.batchV1Api.createNamespacedJob(this.namespace, jobSpec).catch((err) => {
      throw handleKubernetesError(
        "Kubernetes Job creation failed with the following error message:",
        err
      );
    });
  }
  async runJob(jobSpec, taskId, logStream) {
    let timeout;
    const timeoutPromise = new Promise((_, reject) => {
      timeout = setTimeout(
        reject,
        this.timeoutMs,
        new Error(`Failed to complete in ${this.timeoutMs} ms`)
      );
    });
    const { promise: waitPromise, close: waitClose } = this.waitPod(taskId);
    const { promise: tailPromise, close: tailClose } = this.tailLogs(
      taskId,
      logStream
    );
    const taskPromise = Promise.all([
      waitPromise,
      tailPromise,
      this.createJob(jobSpec)
    ]).finally(() => {
      clearTimeout(timeout);
    });
    return Promise.race([timeoutPromise, taskPromise]).finally(() => {
      return waitClose();
    }).finally(() => {
      return tailClose();
    });
  }
}
function handleKubernetesError(message, err) {
  if (err instanceof clientNode.HttpError) {
    return new Error(`${message} ${err.body.message}`);
  }
  return new Error(`${message} ${err}`);
}

class HostDiscovery {
  constructor(impl) {
    this.impl = impl;
  }
  /**
   * Creates a new HostDiscovery discovery instance by reading
   * from the `backend` config section, specifically the `.baseUrl` for
   * discovering the external URL, and the `.listen` and `.https` config
   * for the internal one.
   *
   * Can be overridden in config by providing a target and corresponding plugins in `discovery.endpoints`.
   * eg.
   * ```yaml
   * discovery:
   *  endpoints:
   *    - target: https://internal.example.com/internal-catalog
   *      plugins: [catalog]
   *    - target: https://internal.example.com/secure/api/{{pluginId}}
   *      plugins: [auth, permission]
   *    - target:
   *        internal: https://internal.example.com/search
   *        external: https://example.com/search
   *      plugins: [search]
   * ```
   *
   * The fixed base path is `/api`, meaning the default full internal
   * path for the `catalog` plugin will be `http://localhost:7007/api/catalog`.
   */
  static fromConfig(config) {
    return new HostDiscovery(HostDiscovery$1.fromConfig(config));
  }
  async getBaseUrl(pluginId) {
    return this.impl.getBaseUrl(pluginId);
  }
  async getExternalBaseUrl(pluginId) {
    return this.impl.getExternalBaseUrl(pluginId);
  }
}
class CacheManager {
  constructor(_impl) {
    this._impl = _impl;
  }
  /**
   * Creates a new {@link CacheManager} instance by reading from the `backend`
   * config section, specifically the `.cache` key.
   *
   * @param config - The loaded application configuration.
   */
  static fromConfig(config, options = {}) {
    return new CacheManager(CacheManager$1.fromConfig(config, options));
  }
  forPlugin(pluginId) {
    return {
      getClient: (options) => {
        const result = this._impl.forPlugin(pluginId);
        return options ? result.withOptions(options) : result;
      }
    };
  }
}
class DatabaseManager {
  constructor(_databaseManager, logger) {
    this._databaseManager = _databaseManager;
    this.logger = logger;
  }
  static fromConfig(config, options) {
    const _databaseManager = DatabaseManager$1.fromConfig(config, options);
    return new DatabaseManager(_databaseManager, options?.logger);
  }
  forPlugin(pluginId, deps) {
    const logger = this.logger ?? {
      debug() {
      },
      info() {
      },
      warn() {
      },
      error() {
      },
      child() {
        return this;
      }
    };
    const lifecycle = deps?.lifecycle ?? {
      addShutdownHook() {
      },
      addStartupHook() {
      }
    };
    return this._databaseManager.forPlugin(pluginId, { logger, lifecycle });
  }
}
const isDatabaseConflictError = backendPluginApi.isDatabaseConflictError;
const resolvePackagePath = backendPluginApi.resolvePackagePath;
const resolveSafeChildPath = backendPluginApi.resolveSafeChildPath;
const isChildPath = backendPluginApi.isChildPath;

function cacheToPluginCacheManager(cache) {
  return {
    getClient: (opts) => cache.withOptions(opts)
  };
}

class BackstageLoggerTransport extends Transport__default.default {
  constructor(backstageLogger, opts) {
    super(opts);
    this.backstageLogger = backstageLogger;
  }
  log(info, callback) {
    if (typeof info !== "object" || info === null) {
      callback();
      return;
    }
    const { level, message, ...meta } = info;
    switch (level) {
      case "error":
        this.backstageLogger.error(String(message), meta);
        break;
      case "warn":
        this.backstageLogger.warn(String(message), meta);
        break;
      case "info":
        this.backstageLogger.info(String(message), meta);
        break;
      case "debug":
        this.backstageLogger.debug(String(message), meta);
        break;
      default:
        this.backstageLogger.info(String(message), meta);
    }
    callback();
  }
}
function loggerToWinstonLogger(logger, opts) {
  return winston.createLogger({
    transports: [new BackstageLoggerTransport(logger, opts)]
  });
}

function createTokenManagerShim(auth, config, logger) {
  const tokenManager = ServerTokenManager.fromConfig(config, { logger });
  return {
    async getToken() {
      return tokenManager.getToken();
    },
    async authenticate(token) {
      if (token) {
        const credentials = await auth.authenticate(token);
        if (auth.isPrincipal(credentials, "service")) {
          return;
        }
      }
      await tokenManager.authenticate(token);
    }
  };
}
function createIdentityServiceShim(auth, userInfo) {
  return {
    async getIdentity(options) {
      const authHeader = options.request.headers.authorization;
      if (typeof authHeader !== "string") {
        return void 0;
      }
      const token = authHeader.match(/^Bearer[ ]+(\S+)$/i)?.[1];
      if (!token) {
        return void 0;
      }
      const credentials = await auth.authenticate(token);
      if (!auth.isPrincipal(credentials, "user")) {
        return void 0;
      }
      const info = await userInfo.getUserInfo(credentials);
      return {
        token,
        identity: {
          type: "user",
          userEntityRef: info.userEntityRef,
          ownershipEntityRefs: info.ownershipEntityRefs
        }
      };
    }
  };
}
function makeLegacyPlugin(envMapping, envTransforms) {
  return (name, createRouterImport) => {
    return backendPluginApi.createBackendPlugin({
      pluginId: name,
      register(env) {
        env.registerInit({
          deps: {
            ...envMapping,
            $$router: backendPluginApi.coreServices.httpRouter,
            $$auth: backendPluginApi.coreServices.auth,
            $$userInfo: backendPluginApi.coreServices.userInfo,
            $$config: backendPluginApi.coreServices.rootConfig,
            $$logger: backendPluginApi.coreServices.logger
          },
          async init({
            $$auth,
            $$config,
            $$logger,
            $$router,
            $$userInfo,
            ...envDeps
          }) {
            const { default: createRouter } = await createRouterImport;
            const pluginEnv = Object.fromEntries(
              Object.entries(envDeps).map(([key, dep]) => {
                const transform = envTransforms[key];
                if (transform) {
                  return [key, transform(dep)];
                }
                return [key, dep];
              })
            );
            const auth = $$auth;
            const config = $$config;
            const logger = $$logger;
            const router = $$router;
            const userInfo = $$userInfo;
            pluginEnv.tokenManager = createTokenManagerShim(
              auth,
              config,
              logger
            );
            pluginEnv.identity = createIdentityServiceShim(auth, userInfo);
            const pluginRouter = await createRouter(
              pluginEnv
            );
            router.use(pluginRouter);
          }
        });
      }
    });
  };
}
const legacyPlugin = makeLegacyPlugin(
  {
    cache: backendPluginApi.coreServices.cache,
    config: backendPluginApi.coreServices.rootConfig,
    database: backendPluginApi.coreServices.database,
    discovery: backendPluginApi.coreServices.discovery,
    logger: backendPluginApi.coreServices.logger,
    permissions: backendPluginApi.coreServices.permissions,
    scheduler: backendPluginApi.coreServices.scheduler,
    reader: backendPluginApi.coreServices.urlReader
  },
  {
    logger: (log) => loggerToWinstonLogger(log),
    cache: (cache) => cacheToPluginCacheManager(cache)
  }
);

function createCredentialsWithServicePrincipal(sub, token, accessRestrictions) {
  return {
    $$type: "@backstage/BackstageCredentials",
    version: "v1",
    token,
    principal: {
      type: "service",
      subject: sub,
      accessRestrictions
    }
  };
}
function createCredentialsWithUserPrincipal(sub, token, expiresAt) {
  return {
    $$type: "@backstage/BackstageCredentials",
    version: "v1",
    token,
    expiresAt,
    principal: {
      type: "user",
      userEntityRef: sub
    }
  };
}
function createCredentialsWithNonePrincipal() {
  return {
    $$type: "@backstage/BackstageCredentials",
    version: "v1",
    principal: {
      type: "none"
    }
  };
}
function toInternalBackstageCredentials(credentials) {
  if (credentials.$$type !== "@backstage/BackstageCredentials") {
    throw new Error("Invalid credential type");
  }
  const internalCredentials = credentials;
  if (internalCredentials.version !== "v1") {
    throw new Error(
      `Invalid credential version ${internalCredentials.version}`
    );
  }
  return internalCredentials;
}

class AuthCompat {
  constructor(identity, tokenManager) {
    this.identity = identity;
    this.tokenManager = tokenManager;
  }
  isPrincipal(credentials, type) {
    const principal = credentials.principal;
    if (principal.type !== type) {
      return false;
    }
    return true;
  }
  async getNoneCredentials() {
    return createCredentialsWithNonePrincipal();
  }
  async getOwnServiceCredentials() {
    return createCredentialsWithServicePrincipal("external:backstage-plugin");
  }
  async authenticate(token) {
    const payload = token.split(".").length === 3 ? jose.decodeJwt(token) : void 0;
    if (payload?.aud === "backstage") {
      const identity = await this.identity.getIdentity({
        request: {
          headers: { authorization: `Bearer ${token}` }
        }
      });
      if (!identity) {
        throw new errors.AuthenticationError("Invalid user token");
      }
      return createCredentialsWithUserPrincipal(
        identity.identity.userEntityRef,
        token,
        this.#getJwtExpiration(token)
      );
    }
    await this.tokenManager?.authenticate(token);
    return createCredentialsWithServicePrincipal(
      "external:backstage-plugin",
      token
    );
  }
  async getPluginRequestToken(options) {
    const internalForward = toInternalBackstageCredentials(options.onBehalfOf);
    const { type } = internalForward.principal;
    switch (type) {
      // TODO: Check whether the principal is ourselves
      case "service": {
        if (this.tokenManager) {
          return this.tokenManager.getToken();
        }
        return { token: internalForward.token ?? "" };
      }
      case "user":
        if (!internalForward.token) {
          throw new Error("User credentials is unexpectedly missing token");
        }
        return { token: internalForward.token };
      // NOTE: this is not the behavior of this service in the new backend system, it only applies
      //       here since we'll need to accept and forward requests without authentication.
      case "none":
        return { token: "" };
      default:
        throw new errors.AuthenticationError(
          `Refused to issue service token for credential type '${type}'`
        );
    }
  }
  async getLimitedUserToken(credentials) {
    const internalCredentials = toInternalBackstageCredentials(credentials);
    const { token } = internalCredentials;
    if (!token) {
      throw new errors.AuthenticationError(
        "User credentials is unexpectedly missing token"
      );
    }
    return { token, expiresAt: this.#getJwtExpiration(token) };
  }
  #getJwtExpiration(token) {
    const { exp } = jose.decodeJwt(token);
    if (!exp) {
      throw new errors.AuthenticationError("User token is missing expiration");
    }
    return new Date(exp * 1e3);
  }
  listPublicServiceKeys() {
    throw new Error("Not implemented");
  }
}
function getTokenFromRequest(req) {
  const authHeader = req.headers.authorization;
  if (typeof authHeader === "string") {
    const matches = authHeader.match(/^Bearer[ ]+(\S+)$/i);
    const token = matches?.[1];
    if (token) {
      return token;
    }
  }
  return void 0;
}
const credentialsSymbol = Symbol("backstage-credentials");
class HttpAuthCompat {
  #auth;
  constructor(auth) {
    this.#auth = auth;
  }
  async #extractCredentialsFromRequest(req) {
    const token = getTokenFromRequest(req);
    if (!token) {
      return this.#auth.getNoneCredentials();
    }
    return this.#auth.authenticate(token);
  }
  async #getCredentials(req) {
    return req[credentialsSymbol] ??= this.#extractCredentialsFromRequest(req);
  }
  async credentials(req, options) {
    const credentials = await this.#getCredentials(req);
    const allowed = options?.allow;
    if (!allowed) {
      return credentials;
    }
    if (this.#auth.isPrincipal(credentials, "none")) {
      if (allowed.includes("none")) {
        return credentials;
      }
      throw new errors.AuthenticationError("Missing credentials");
    } else if (this.#auth.isPrincipal(credentials, "user")) {
      if (allowed.includes("user")) {
        return credentials;
      }
      throw new errors.NotAllowedError(
        `This endpoint does not allow 'user' credentials`
      );
    } else if (this.#auth.isPrincipal(credentials, "service")) {
      if (allowed.includes("service")) {
        return credentials;
      }
      throw new errors.NotAllowedError(
        `This endpoint does not allow 'service' credentials`
      );
    }
    throw new errors.NotAllowedError(
      "Unknown principal type, this should never happen"
    );
  }
  async issueUserCookie(_res) {
    return { expiresAt: new Date(Date.now() + 36e5) };
  }
}
class UserInfoCompat {
  async getUserInfo(credentials) {
    const internalCredentials = toInternalBackstageCredentials(credentials);
    if (internalCredentials.principal.type !== "user") {
      throw new Error("Only user credentials are supported");
    }
    if (!internalCredentials.token) {
      throw new Error("User credentials is unexpectedly missing token");
    }
    const { sub: userEntityRef, ent: ownershipEntityRefs = [] } = jose.decodeJwt(
      internalCredentials.token
    );
    if (typeof userEntityRef !== "string") {
      throw new Error("User entity ref must be a string");
    }
    if (!Array.isArray(ownershipEntityRefs) || ownershipEntityRefs.some((ref) => typeof ref !== "string")) {
      throw new Error("Ownership entity refs must be an array of strings");
    }
    return { userEntityRef, ownershipEntityRefs };
  }
}
function createLegacyAuthAdapters(options) {
  const {
    auth,
    httpAuth,
    userInfo = new UserInfoCompat(),
    discovery
  } = options;
  if (auth && httpAuth) {
    return {
      auth,
      httpAuth,
      userInfo
    };
  }
  if (auth) {
    return {
      auth,
      userInfo
    };
  }
  if (httpAuth) {
    return {
      httpAuth,
      userInfo
    };
  }
  const identity = options.identity ?? pluginAuthNode.DefaultIdentityClient.create({ discovery });
  const authImpl = new AuthCompat(identity, options.tokenManager);
  const httpAuthImpl = new HttpAuthCompat(authImpl);
  return {
    auth: authImpl,
    httpAuth: httpAuthImpl,
    userInfo
  };
}

exports.CacheManager = CacheManager;
exports.DatabaseManager = DatabaseManager;
exports.DockerContainerRunner = DockerContainerRunner;
exports.Git = Git;
exports.HostDiscovery = HostDiscovery;
exports.KubernetesContainerRunner = KubernetesContainerRunner;
exports.ServerTokenManager = ServerTokenManager;
exports.SingleHostDiscovery = HostDiscovery;
exports.cacheToPluginCacheManager = cacheToPluginCacheManager;
exports.coloredFormat = coloredFormat;
exports.createConfigSecretEnumerator = createConfigSecretEnumerator;
exports.createLegacyAuthAdapters = createLegacyAuthAdapters;
exports.createRootLogger = createRootLogger;
exports.createServiceBuilder = createServiceBuilder;
exports.createStatusCheckRouter = createStatusCheckRouter;
exports.errorHandler = errorHandler;
exports.getRootLogger = getRootLogger;
exports.getVoidLogger = getVoidLogger;
exports.isChildPath = isChildPath;
exports.isDatabaseConflictError = isDatabaseConflictError;
exports.legacyPlugin = legacyPlugin;
exports.loadBackendConfig = loadBackendConfig;
exports.loggerToWinstonLogger = loggerToWinstonLogger;
exports.makeLegacyPlugin = makeLegacyPlugin;
exports.notFoundHandler = notFoundHandler;
exports.redactWinstonLogLine = redactWinstonLogLine;
exports.requestLoggingHandler = requestLoggingHandler;
exports.resolvePackagePath = resolvePackagePath;
exports.resolveSafeChildPath = resolveSafeChildPath;
exports.setRootLogger = setRootLogger;
exports.statusCheckHandler = statusCheckHandler;
exports.useHotCleanup = useHotCleanup;
exports.useHotMemoize = useHotMemoize;
//# sourceMappingURL=index.cjs.js.map
